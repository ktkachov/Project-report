Frame 1:
Hello, my name is Kyrill Tkachov and in my project I explore the acceleration of unstructured mesh computations using
customised streaming hardware accelerators implemented on FPGAs.

Frame 2:
Unstructured meshes are a common technique that is used to discretise systems of PDEs in many scientific simulations.
They allow the definition of a grid that can be refined and coarsened where needed to save space and provide more accuracy.
The sample application I try to accelerate is called Airfoil and I will briefly present its main features now.

Frame 3:
The mesh we are dealing with has 3 elements: nodes, cells and edges. The unstructuredness comes from the use of indirection maps to
explicitly store connectivity information as shown in this diagram. For example: the edge-to-node map shows that edge number 3 connects
nodes 2 and 4.

Frame 4:
Associated with the mesh elements are some data sets that represent some physical quantity, like node coordinates or flux.
These data sets may be of multiple dimension, for example: the node coordinate set "x" is of dimension 2 (for x and y coordinates), that is a pair
of real numbers.

Frame 5:
The computation over the mesh is performed by a number of kernels that are applied in iterations over some mesh element. The kernel we are interested
in is called res_calc and it is applied to the edges of a mesh. res_calc reads the x,q and adt data sets associated with two nodes and cells and updates the 
res set of the two cells.

Frame 6:
This here is the definition of res_calc in C. Do not worry about the meaning of the calculations, just take note that it is mostly floating point calculations.

Frame 7:
The most important facts about res_calc are shown here. It is applied in an iteration over edges. Processing each edge requires two nodes and two cells.
The result is updates a current result. Through various benchmarks it has been shown that res_calc is the most computationally intensive kernel in Airfoil,
therefore it is the one I focus on accelerating.

Frame 8:
The difficulty in accelerating such computations is demonstrated in the application of res_calc to and edge, shown here. The edge and ecell arrays are
indirection maps from edges to nodes and cells respectively. In order to find the two nodes and cells that an edge references we must look them up in the
indirection maps and since these maps are, from a static compilers' point of view, random, the memory accesses make it very hard to enforce some kind of
locality that is necessary for high-performance simulations. Therefore this is what makes it an interesting research problem.

Frame 9:
Now, what do I mean by a custom streaming architecture? Consider the example of trying to compute the function r = x^2 + y - sqrt(z). In a conventional CPU, like the
one on the right the computation must be expressed as a sequence of instructions that have to be decoded by one unit, have their operands fetched by another 
and then computed by a third, writing intermediate results to a register file. What if we are trying to compute this function for an array of a million r's?
Because of the cache misses and the data hazards, even with sophisticated techniques like out-of-order execution and value forwarding the throughput would be quite low.
Enter the streaming architecture. On the left, we see how the functional units for the arithmetic operations are custom-wired to compute the function we want and nothing else.
Now we can pipeline them as much as we can and, providing that we can keep the pipeline full, we get a throughput of one result for r per cycle!. There is no register file, no decoding.
Furthermore, the more complex the arithmetic operations get, the more cycles the CPU will take to complete, whereas in a custom pipeline the pipeline gets deeper but the throughput remains
the same! Asymptotically speaking, the custom pipeline must always win in performance.

Frame 10:
The hardware that I use to implement the accelerator is the MAX3 card. It is a PCIe card that can be plugged onto the motherboard as any graphics card. The card itself
contains an FPGA that is a piece of reconfigurable hardware. It can be thought of as a large matrix of logical elements, gates and memory cells that can be wired together
in any number of ways. The card also contains a large chunk of DRAM memory that can be accessed with a high bandwidth of 38GB/s from the FPGA, PROVIDED IT IS ACCESSED IN A WELL-BEHAVED
PATTERN, for example a linear pattern. It DOES NOT provide good bandwidth if it is accessed randomly. The host machine can communicate with the card via the PCIe bus that has
a much lower bandwidth of 2GB/s. The FPGA itself provides about 4MBytes of memory called block RAM memory. This memory is on the FPGA itself and it can be accessed in any pattern
in just two cycles. 
This setup gives us the first indications of where to store what. Since the DRAM cannot be accessed randomly we cannot use any mesh connectivity information to access it.
We can, however, access the block RAMs randomly. But our meshes are really big. The test mesh I use has about 720000 cells, which will not fit in the BRAMs, but we can store it in the
DRAM. So I make the decision to store the whole mesh in the DRAM, copy small parts of it into the block RAM and then use the connectivity information to process it on the FPGA. For this
I need to break the mesh into partitions.

Frame 11:
I partition the mesh into chunks that can fit on the block RAM of the FPGA using a widely used tool called METIS. Since in res_calc each edge updates two cells, there will
invariably be some cells that belong to different partitions that are being processed by an edge. We call these cells and nodes the "halo region", shown here in blue.
The halo regions present a problem, because if we process one partition at a time, the res value of a halo cell after the processing of a partition will be only a partial result.
It will still need the increments produced by the other partitions. Therefore I make the decision to make the processing of the halo cells on the host. This is achieved by
sepating halo from non-halo data and receiving the increments from each partition for the halo cells through PCIe and summing them up in the end. The halo data will be kept in separate
block RAMs on the FPGA.

Frame 12:
Here is a diagram of the resulting architecture. The node and cell data are streamed in from the DRAM and from PCIe and stored in block RAMs. After a partition has been streamed in, we process it
by streaming in the edge data. The edge data is just a vector of addresses into the node and cell RAMs that correspond to the two cells and two nodes that the edge references. The iteration
over the edges is transformed into a stream of block RAM addresses. After some control logic to select whether to read from the halo or non-halo RAMs, the data is fed into the arithmetic pipeline
that computes the increments that must be added into the current value of res that is stored in the block RAM at the bottom. This increment operation at the bottom creates a loop. After processing all
of the edges in the partition we stream the res set from the result RAMs out to DRAM and PCIe. Remember that the halo results are just partial results from this partition and it is the hosts' responsibility
to gather all the increments from all the partitions and sum them up properly.

Frame 13:
Now, this approach of streaming in node and cell data for a partitino, processing the partition and writing out the result has a drawback. During the processing stage, we are not utilising the bandwidth of the memory
system! To fix this, I introduce a second level of partitioning. We break up each partition into two "micro-partitions". The idea is that if we finish streaming in the first micro-partition, we can start processing it
immediately while streaming in the second micro-partition. Conversely, while we are processing the second micro-partition we can write out the first one. Notice the intra-partition halo that arises from edges that access
cells from both micro-partition. Care must be taken to not overwrite that region and not write it out until bot micro-partitions have been processed.

Frame 14:
This refinement leads us to an architecture like the one shown in this diagram. In order to keep track of the processing and I/O status of the partitions and micro-partitions involved I add a state machine
that has a number of counters that control the enable signals of the inputs and outputs, as well as the write enable signals on the RAMs. In order to keep track of the partitions, the state machine needs to know
the sizes of each part of the partitions, therefore I add another input to the design that gives these numbers to the state machine.

Frame 15:
This architecture presents the execution pattern shown in the diagram. The accelerator has 3 phases. In the first one we read in the first micro-partition and also write out the second micro-partition from the
previous partition. In the second phase we read in the second micro-partition and simulatenously process the first one. In the third stage we write out the first micro-partition while processing the second.
Notice how the I/O and execution overlaps and the memory system is kept occupied constantly.

Frame 16:
Now it is my job to convince you that this appoach is worth pursuing and warrants a sample implementation. To do this I devise an analytical performance model that is used to predict the performance of the
accelerator. I calculate the performance in terms of architecture and mesh parameters such as clock frequency, number of arithmetic pipelines, memory bandwidth, partition sizes etc. Here I present to you the
concepts on which the model is based. 
For each of the phases in the accelerator the time spent is dominated by the maximum of 3 times. The time to transfer data from DRAM, time to transfer data from PCIe and the time taken to complete the clock
cycles. The first two are just a simple matter of dividing the amount of data we are transferring by the memory bandwidth. The third time during the processing phase is the number of edges we are processing
divided by the number of edges we are processing each cycle, i.e. the number of arithmetic partitions and divided again by the clock frequency. Notice that because of the one-result-per-cycle throughput
of our custom pipelines, the complexity of arithmetic calculations in the pipeline does not play any role!

Frame 17:
Having defined a performance model, we can now explore a whole design space of architectures for the accelerator by plugging in different values for any of the parameters and checking the predicted runtime.
That way we can find out whether there are any architectures out there worth our time.

Frame 18:
Lets first start with the number of pipelines. This is the number of edges we process per cycle. The graph here shows the predicted performance of an architecture runnning at 240MHz with a varying
number of pipelines. For comparison, I ran an OpenCL implementation of Airfoil on the Tesla M2050 and recorded the time taken to execute res_calc, shown here in blue. We can see that by using 4
pipelines or more we can beat the 448-core GPU about 30%! The dramatic increase in performance that comes with the extra pipelines is due to the fact that for my test mesh, the number of edges 
is much larger than the amount of cells and nodes and therefore edge processing in the 2nd and 3rd phases dominates the runtime. Notice that after 7 or so pipelines we no longer get any increase
in performance. That is because we are processing edges so fast that the transferring of the node and cell data through the DRAM and PCIe channels becomes dominant and processing edges even faster
does nothing to alleviate that.

Frame 19:
Next, we focus on the effects of varying the clock frequency. The results of that experimentation are shown here. We see that increasing the clock frequency does not offer the same
boost in performance that arithmetic pipelines offer, but is still a valid way to get more performance out of the hardware.

Frame 20:
An interesting direction to explore is the effect of partition size on performance. The smaller the partition size, the larger is the ratio of non-halo data to halo data in a partition.
If the ratio becomes too small, the time to transfer time of the halo data from PCIe  will become dominant and all other systems will stall while waiting for the PCIe channel to transfer
its data. Ineed, as we can see from this graph, for partition sizes less than 2048 in our sample mesh the execution time rises steeply. Note that the horizontal axis is on a logarithmic scale.
It turns out that 2048 is also the breaking point, below which the halo transfer time becomes larger than the DRAM tansfer time.

Frame 21:
I am now going to talk about a couple of issues that occured when I was implementing the FPGA accelerator and the host code.
The first one is edge dependencies in the accumulator pipeline. Recall the loop that occurs when we add the increments produced by the pipeline to the existing value stored in
the block RAM. The sequence of events is: read current value, pass it through 4 stages of the adder and write the result back to the same address. Now we do this every cycle and
therefore at any given time there are a number of cell addresses being processed in this part of the architecture. Now say that an edge updates a cell at address alpha0, and 3 cycles
later another edge wants to update the alpha0 edge. If the second edge reads the current value before the previous edge has commited its increment, then the value it reads will be the 
wrong one, creating a data dependency, similar to a pipeline hazard that I'm sure you've seen in architecture diagrams many times before. The dependency width here is shown with lambda
and in this example it is equal to 6.

Frame 22:
I avoid this problem by imposing an order on the execution of the edges, a so called "schedule". To do this we first partition each micro-partition again (for a 3rd time) into
"edge-partitions". From this partitioning we construct an "adjacency graph" where the nodes represent the edge partitions and two nodes are connected if the corresponding edge-partitions
update a cell in each other. Next we schedule the graph, that is, we put the nodes in an array such that in a window of width lambda around each node, no node is adjacent to it.

Frame 23:
I'll talk about how that is done in a minute, for now assume we have such a schedule of the graph. We can now create a schedule for the edges by sweeping over the graph schedule and
picking one edge in turn, going upwards. Unfortunately METIS does not provide perfectly even partitions, so there will be "gaps" higher up in the schedule. I mitigate this problem by
introducing no-op edges, dummy edges whose results will be ignored by the accelerator and are just there to preserve the independency constraint in the pipeline. Of course, we want to
add as few no-op edges as possible.

Frame 24:
Now, how to actually schedule the graph? If we take the dual of the adjacency graph, that is connect two nodes if they are NOT connected in the adjacency graph, then the scheduling problem
can be formulated as a Hamiltonian path problem with the extra constraint that nodes in a window of width lambda around each node must be adjacent. Now, the hamiltonian path problem is
known to be NP-hard and therefore any the best algorithm I could come up for scheduling the graph turned out to be not much better than a search through the space of permutations of the
nodes and hence factorial in its complexity. An example run on a graph with 300 nodes and a window-width of 18 did not terminate even after an overnight run.

Frame 25:
To overcome this complexity I use the concept of graph colouring. A coloured graph is an assignment of a number or colour to each node such that no two adjacent nodes share
the same colour. This almost exactly what we want and therefore I colour the adjacency graph. Next, since nodes with the same colour are non-adjacent and guaranteed to be
independent we can put them one after the other in the schedule. Now the trickiness comes when we want to skip to the next colour. I do this by introducing so-called
"no-op edge partitions", edge partitions comprised entirely out of no-op edges that do not conflict with any edge and can therefore be inserted at any point in the schedule.
We can therefore insert lambda no-op edge partitions after each colour group to ensure the independency between two colours and that is our schedule. Obviously, we want to insert
as few no-op edge-partitions as possible. Now optimal colouring is still NP-hard but I can use a sub-optimal but efficient algorithm to do this in polynomial time.

There were quite a few other issues I had to overcome due to hardware limitations but I'll talk about them in the end if I have time.

Frame 26:
Unfortunately my hardware design produces wrong arithmetic results. Through extensive simulation and debugging I tracked the reason down
to NaN results from the no-op edges being commited to the result RAMs. I believe this is a bug with the write-enable and address translation
logic. However, I confirmed that the FPGA reads the correct amount of data, produces the correct amount of data and processes the edges in the
correct order, therefore I claim that the performance results can still be trusted, since it depends entirely on the movement and consumption of data,
which I have confirmed is correct.

Frame 27:
Before I show you the results for the hardware accelerator I want to talk a bit about no-op edges and scheduling again. Remember, there are two sources for adding
no-op edges: no-op edge-partitions and uneven partitioning by METIS. Intuition tells us that the more edge-partitions we divide a micro-partition into, the smaller those
edge-partitions will be, therefore the no-op edge partitions will also be smaller and hence fewer no-op edges will be added on their behalf. As we can see in the red graph,
this is verified, except that for larger number of partitions, we get these fluctuations. My hypothesis for these was that they are caused by sub-optimal behaviour by METIS.
To test this I ran the scheduling code on the same mesh with a window-width of 0, that is imposing no independency constraing and eliminating any no-op partitions, thus guaranteeing
that all no-edges would be a result of METIS. We see that the fluctuation pattern is pretty much identical, thus verifying my suspicion. Since runtime in my sample 1-pipeline
design is expected to be dominated by edge processing, we would want to experiment a bit to find a setting that does not trigger a pathologically bad partitioning by METIS.

Frame 28:
I built my sample design at clock frequencies ranging from 100 to 180 MHz and compared their runtimes to the runtime predicted by the performance model. As we can see in this graph
the two predicted and the measured runtimes are very close to each other, with the measured time being slightly larger due to issues unaccounted for in model, such as stream setup time.

Frame 29:
This is a fantastic result in my opinion because the performance model has been validated and therefore we can use it to explore the architecture design space to find the ones that
are interesting and also find out how various architecture paremeters affect performance.
The reason we can predict the performance so accurately is because of the simple memory hierarchy. There is only the DRAM accessed in a completely predictable pattern and the block RAMs
which, due to the mesh layout and partitioning, will never suffer from cache misses.
The model shows that architectures in this family can indeed achieve interesting speedup, rivalling a 400-core GPU implementation with only 4-5 pipelines, running at a fraction of the
clock frequency.
I am happy with the results of this project because it was not evident from the start that this approach would be a good idea, given that the main large memory, the DRAM, cannot be
accessed in the random pattern that is dictated by the format of unstructured meshes. I managed to overcome that by designing a layout for the mesh, as well as specifying a family
of architecture, custom built for each other.

Frame 30:
This project has a lot of potential for further work, apart from getting the arithmetic results right. The most important of them is the acceleration of the other Airfoil kernels.
This a tricky one because other kernels may not iterate over edges like res_calc and can therefore require a different data layout that may not be compatible wit the one for
res_calc.
Another interesting thing to do would be to create a compilation system of sorts where you can plug in the parameters of the performance model and have it create for you
the host code and the accelerator design.