\documentclass[11pt]{report}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{tabularx}
\usepackage{tikz}
\usepackage{float}
\usepackage{algpseudocode}
\usetikzlibrary{arrows}
\usetikzlibrary{calc}
\usetikzlibrary{matrix}
\usetikzlibrary{shapes,snakes}
\usetikzlibrary{shapes.multipart}
\usepackage{scalefnt}
\usepackage{minted}
\usepackage{amsmath}
\usepackage[linktocpage]{hyperref}

\title{Accelerating Unstructured Mesh Computations using a Custom Streaming Architecture}
\author{Kyrylo Tkachov\\\\ Supervisor: Professor Paul Kelly\\\\Second marker: Dr. Tony Field.}
\hypersetup{pdfborder = {0 0 0}}
\begin{document}
\begin{titlepage}
\begin{center}
{\Huge Accelerating Unstructured Mesh Computations using a Custom Streaming Architecture} \\
\begin{verbatim}


\end{verbatim}


{\Large Kyrylo Tkachov} \\
\begin{verbatim}

\end{verbatim}

{\Large Supervisor: Prof. Paul Kelly} \\ 
\begin{verbatim}

\end{verbatim}

{\Large Second marker: Dr. Tony Field} \\ 
\begin{verbatim}

\end{verbatim}

{\scalefont{0.1}
\begin{tikzpicture}[scale=0.3]
\newcommand{\firstiph}[1] {
  \fill [green!10!white] #1 rectangle +(3, -2);
  \fill [red!10!white] ($#1 + (0,-1.5)$) rectangle ($#1 + (3, -2)$);
  \draw [thick] #1 rectangle +(3, -2);
  \draw [thick] ($#1 + (0,-1.5)$) rectangle ($#1 + (3, -2)$);
  \draw [thick] #1 -- +(0, -2.4);
  \draw [thick] ($#1 + (3, 0)$) -- +(0, -2.4);
  \node at ($#1 + (1.5, -1)$) {$\mu$p 1};
  \node at ($#1 + (1.5, -1.7)$) {IPH};
}
\newcommand{\second}[1] {
  \fill [green!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);

  \node at ($#1 + (1.5, -1.5)$) {$\mu$p 2};
  \draw [thick] #1 -- ++(0, -2.4) -- ++(3, 0) -- ++(0, 2.4);
}
\newcommand{\secondiph}[1] {
  \fill [green!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \fill [red!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -0.9)$);
  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -0.9)$);

  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \node at ($#1 + (1.5, -0.65)$) {IPH};
  \node at ($#1 + (1.5, -1.5)$) {$\mu$p 2};
  \draw [thick] #1 -- ++(0, -2.4) -- ++(3, 0) -- ++(0, 2.4);
}
\newcommand{\first}[1] {
  \fill [green!10!white] #1 rectangle ($#1 + (3, -2)$);
  \draw[thick] #1 rectangle ($#1 + (3, -2)$);
  \draw [thick] ($#1 + (0, -2.4)$) -- #1 -- ($#1 + (3, 0)$) -- ($#1 +(3, -2.4)$);
  \node at ($#1 + (1.5, -1)$) {$\mu$p 1};

}

\draw (2.5, 21) node {\emph{READ}};
\draw (7.5, 21) node {\emph{PROCESS}};
\draw (12.5, 21) node {\emph{WRITE}};
\draw [very thick] (-2.5, 20.5) -- (15, 20.5);

\draw [dashed, very thick] (5, 21.5) -- (5, 0);
\draw [dashed, very thick] (10, 21.5) -- (10, 0);
\secondiph{(11, 20)};
\firstiph{(1,20)};
\second{(1, 17)};
\firstiph{(6,17)};
\secondiph{(6, 14)};
\first{(11, 14)};
\firstiph{(1,11)};
\secondiph{(11, 11)};
\second{(1, 8)};
\firstiph{(6, 8)};
\secondiph{(6, 5)};
\first{(11, 5)};
\firstiph{(1, 2)};
\secondiph{(11, 2)};
\draw [dashed] (-2.5, 17.2) -- (15, 17.2);
\draw [dashed] (-2.5, 14.2) -- (15, 14.2);
\draw [dashed] (-2.5, 11.2) -- (15, 11.2);
\draw [dashed] (-2.5, 8.2) -- (15, 8.2);
\draw [dashed] (-2.5, 5.2) -- (15, 5.2);
\draw [dashed] (-2.5, 2.2) -- (15, 2.2);
\draw [very thick] (0, 21.5) -- (0, 0);
\draw (-1.2, 18.5) node {\emph{ST 1}};
\draw (-1.2, 15.5) node {\emph{ST 2}};
\draw (-1.2, 12.5) node {\emph{ST 3}};
\draw (-1.2, 9.5) node {\emph{ST 1}};
\draw (-1.2, 6.5) node {\emph{ST 2}};
\draw (-1.2, 3.5) node {\emph{ST 3}};
\draw (-1.2, 0.5) node {\emph{ST 1}};

\draw [->,densely dashed, very thick] (-2.9, 21) -- (-2.9, 0);
\draw (-3.2, 10.5) node[rotate=90]{\emph{TIME}};


\newcommand{\redpart}[1]{
\draw [red, very thick, rounded corners] #1 -- ++(3.75, 0) -- ++(0, -3.15) -- ++(5.6, 0) -- ++(0, -3) -- ++(4.4,0) -- ++(0, -5.7)
-- ++(-4, 0) -- ++(0, 3) -- ++(-4.5, 0) -- ++(0, 3) -- ($#1 + (0, -5.9)$) -- cycle;
}

\redpart{(0.5, 20.25)};
\redpart{(0.5, 11.25)};

\draw [red, very thick, rounded corners] (0.2, 2.1) -- ++(4, 0) -- ++(0, -2.6);
\draw [red, very thick, rounded corners] (10.5, 20.25) -- ++(0, -2.9) -- ++(4, 0);

\end{tikzpicture}
}
\begin{verbatim}

\end{verbatim}
{\Large 2012, Department of Computing}
\begin{verbatim}

\end{verbatim}
{\Large Imperial College London}
\end{center}
\end{titlepage}
\pagestyle{headings}
\begin{abstract}
In this report we present a methodology for accelerating computations performed on 
unstructured meshes in the course of a finite volume approach. We implement a custom streaming datapath using
Field Programmable Gate Arrays, or FPGAs to perform the bulk of the floating point 
operations required by the application. In particular, we focus on dealing with irregular
 memory access patterns that are a consequence of using an unstructured mesh in such a way as
to facilitate a streaming model of computation. We describe the partitioning of the mesh and 
the techniques used to exchange information between neighbouring partitions, using so-called halos. We provide an implementation of a concrete 2D finite volume application and consider
the extension to 3D and more complex computations.
We evaluate our results by comparing the speedup achieved with analogous GPGPU and multi-core
 processor implementations.
\end{abstract}
\section*{Acknowledgements \centering}
I would like to thank Professor Paul Kelly for giving me so much of his time and ideas and 
making me aware of scope of the project and the intricacies involved. I would also like to
 thank Dr. Carlo Bertolli for providing practical advice and explaining the labyrinth that is
heterogenous computing.
Special thanks go to the team at Maxeler Technologies for helping me out with the details
of FPGA-based acceleration and providing support for their excellent toolchain
I extend my gratitude to Dr. Tony Field, my personal tutor, who supported me throughout my 
years at Imperial College and guided so much of my academic development, as well as being
the second supervisor on this project.

I would like to thank my mother and grandfather for supporting me through university, both 
materially and psychologically.
Last but not least, I would like to thank my coursemates and friends all over the world,
with whom I've had many thought-provoking discussions on every subject imaginable and who
always kept me motivated, even when I doubted myself.
\newpage
\tableofcontents

\chapter{Introduction}
This project presents a methodology for accelerating computations performed on unstructured
meshes in the context of Computational Fluid Dynamics (CFD). We use Field Programmable Gate
Arrays, or FPGAs, to construct a high-throughput streaming pipeline which is kept filled thanks
to an appropriate data layout and partitioning scheme for the mesh. We explore the rearrangement
and grouping schemes used to achieve locality of the data points. A formal performance
model is constructed to predict the performance characteristics of our architecture and hence
justify the design choices made. Appropriate evaluation
tests are performed to evaluate the results on a sample CFD application, achieving speedup
comparable with state of the art GPGPU and multi-processor solutions. 
In this section we present a general overview of the problem domain, the hardware platform and the
contributions of this project.

\section{The domain}
Computational Fluid Dynamics, or CFD, is a branch of physics focused on numerical algorithms
that simulate the movement of fluids and gases and their interactions with surfaces. These 
simulations are widely used by engineers to design structures and equipment that interact
with fluid substances, for example airplane wings and turbines, water and oil pipelines etc.

The required calculations are usually expressed as systems of partial differential equations,
the Navier-Stokes equations or the Euler equations,
which are discretized using any of a number of techniques. The technique used by our sample
application, Airfoil, is the finite volume method that calculates values
at discrete places in a mesh and relies on the observation that the fluxes entering a volume
are equal to the fluxes leaving it. This project is not concerned with the exact mathematical
formulation of these techniques, but they provide a feel for the origins of the problem 
domain.

\section{The Airfoil program}
The sample program we examine is called Airfoil, a 2D unstructured mesh finite volume simulation
of fluid motion around an airplane wing (which has the shape of an airfoil). Airfoil was
written as a representative of the class of programs that are tackled by OP2, a framework
partially developed and maintained by the Software Performance Optimisation group at Imperial 
College to abstract the acceleration of unstructured mesh computations on a wide variety of 
hardware backends.

Airfoil defines an unstructured mesh through sets of nodes, edges and cells and associating 
them through mappings. Airfoil is written in the C language and these sets are represented at
 the lowest level as C-arrays. Then data is associated with these sets, such as node coordinates, temperature, pressure etc.
 The mesh solution is then expressed as the
 conceptually parallel application of computational kernels on the data associated with each
 element of a particular set (nodes, edges, cells). These kernels are usually floating point-
intensive operations and update the datasets. The procedure is repeated through multiple iterations as desired until
a steady-state solution is reached. A more detailed discussion of the unstructured mesh is presented in the Background
section of this report.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.75\textwidth]{pics/airfoil_mesh.png}
  \caption{Visualisation of a reduced version of the Airfoil mesh  \label{fig:airfoil_mesh}}
 
\end{figure}

\section{FPGAs, streaming and acceleration}
In this project we explore the acceleration possibilities of problems in the described domain
 by using Field Programmable Gate Arrays, or FPGAs. FPGAs are integrated circuits that can
be reconfigured on the fly to implement in hardware any logic design. Thanks to this property
they provide the development flexibility of software with the benefits of an explicit custom
hardware datapath. At a high level, FPGAs can be viewed as a two-dimensional grid of logic
elements that can be interconnected in any desirable way.

The FPGA acceleration approach we look at is the streaming model of comutation. In a 
streaming approach we create a dataflow graph out of simple computational nodes that perform 
a specific operation on pieces of data pushed in and out of them. Connecting these nodes
together creates a pipeline through which one can stream an array of data and get one output
per cycle thus achieving high throughput. A simple dataflow graph can be seen in 
figure~\ref{fig:graph_simple}.
FPGAs are usually programmed using a low level hardware description language like VHDL or
 Verilog, however many tools have been designed that allow a developer to specify high-level designs.
 We use MaxCompiler, a compiler that lets
us specify the computational graph through a high-level Java API, so we focus on the
functional aspects of our design and the tool generates a hardware implementation of it.
We use this approach to implement a datapath the kernel described
in Airfoil and we then look at approaches to utilise the streaming bandwidth. The 
FPGAs we consider have a large DRAM storage area attached ($>$24GB) to them that can be used to store
the mesh and utilising the bandwidth of that DRAM fully is key to achieving maximum 
performance.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.2\textwidth]{pics/graph_simple.png}
  \caption{A simple dataflow graph that implements the function $y(x) = x^2 + x$.
   \label{fig:graph_simple}}
\end{figure}

During the course of our work it emerges that in order to stream data to and from the 
accelerator continuously, we need to enforce some spatial locality in the mesh data, thus
requiring us to reorder the data and organise it into partitions that will be stored in
the kernel internally and will need to exchange data with neighbouring partitions through
a halo exchange mechanism. This opens a whole new space of decisions that we must make 
pertaining to the storage layout and streaming responsiblities of the DRAM and the host 
machine. We present the mesh partitioning schemes that are used to maximise DRAM bandwidth
utilisation and maximise pipelining.

We present a performance model that will be used to describe the theoretical performance
increase of the system in terms of various parameters like DRAM utilisation, clock frequency etc.
Finally we evaluate the performance of our implementation of Airfoil against existing
GPGPU and multi-processors cluster implementations.

\section{Contributions}
\begin{itemize}
\item We present a methodology for accelerating unstructured mesh computations using deeply
pipelined streaming FPGA designs.
\item We investigate memory layout issues that arise from efforts to maximise the spatial
locality of the mesh.
\item We provide a hardware accelerated version of part of the Airfoil program using the methodologies 
described in this report.
\item We provide a predictive performance model that is used to justify our design decisions
and provide a formal expression of the potential speedup.
\item We investigate the potential for generalisation of the problem and the acceleration of
more complex industry-grade unstructured mesh simulations.
\end{itemize}

\chapter{Background}
This section provides more detail on the sample application, the representation of meshes,
the data sets and the iteration structure of Airfoil. An overview of the Maxeler toolchain
is given, which is used to implement the streaming solution we develop. The streaming model of
computation is presented in the context of MaxCompiler by walking through steps to build a simple
MaxCompiler application. Real number representation is discussed and the concept of a halo is introduced.
 Previous work in this area is presented and summarised in order to provide a context for the contributions of our work.
\section{Unstructured meshes and their representation}
The spatial domain of the problem can be discretised into either a structured or an 
unstructured mesh. A structured mesh has the advantage of having a highly regular structure
and thus a highly predictable access pattern. If, however, one needs a more detailed solution
around a particular area, the mesh would would have to be fine-grained across the whole 
domain, thus increasing the number of cells, nodes and edges by a large factor even in areas
that are not of such great interest. This is where unstructured meshes come in. They 
explicitly describe the connectivity between the elements and can thus be refined and 
coarsened around particular areas of interest. This provides much greater flexibility at the
expense of losing the regularity of the mesh, forcing us to store the connectivity information
that defines its topology. It is useful to have an intimate understanding of the representation
of unstructured meshes in order to understand the techniques discussed further on. A graphical 
example is shown in figure \ref{fig:mesh_small_example}

In our sample application the mesh distinguishes three main elements: nodes, cells and edges.
We have to represent the connectivity information between them. This is done through
\emph{indirection maps} that store, for example, the nodes that an edge connects or the
nodes that a cell contains. In the application we explore the cells always have four nodes
and the edges always connect two nodes and have two cells adjacent. In the more general case
 of variable-dimension cells (quadrilaterals, triangles, hexagons all mixed together) we would need
an additional array storing the indices into the indirection maps and the sizes of the elements.
But we do not consider such meshes here.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.95\textwidth]{pics/mesh_small.png}
  \caption{An example mesh and its representation using indirection arrays. The cell numbers are shown inside
  the quadrilaterals formed by the nodes (circles) and edges (edges connecting the nodes). Together with the
  indirection map, we also store an integer $dim \in \mathbb{N}$ which specifies the dimension of the mapping.
  Thus, the data associated with element $i$ are stored in the range $[i*dim$, ... , $i*(dim+1)-1]$ of the relevant indirection
  map (in the example: the nodes associated with edge 3 are stored at indices $3*2=6$ and $3*2+1=7$). Note: in the edge-to-cell map
  $-1$ represents a boundary cell that may be handled in a special way by a computational kernel.
\label{fig:mesh_small_example}}
\end{figure}

The above method deals with the connectivity information amongst the different elements of the mesh.
The data on which we perform the actual arithmetic calculations is stored in arrays indexed by element number.
Such an approach is presented in figure \ref{fig:data_set_ex}.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.60\textwidth]{pics/data_set_ex.png}
  \caption{An example mesh with coordinate data associated with each node ($(x,y)$ from $node\_id~(x,y)$).
  The coordinate data will be represented as an array of floating point numbers
  $x = \{0.0,0.0, 2.78,5.0, 3.0,1.0, 3.0,6.7, 7.5,3.14, 9.0,7.7\}$.
  Again we also record the dimension of the data (in this case $dim=2$) in order to access the data set associated with each
  element. In this example, the coordinate data for node 4 is stored at indices $4*2=8$ and $4*2+1=9$ of the array $x$.
\label{fig:data_set_ex}}
\end{figure}

\section{Airfoil}
Airfoil was written as a representative of the class of problems we are interested in.
It was initially designed as a non-trivial example of the issues tackled by the OP2 framework.
Although we are not directly dealing with OP2 in this project, an overview of Airfoil within
this context is provided by MB Giles et al \cite{OP2_presentation} because it discusses the acceleration
issues arising from the memory access pattern.

The computational work in Airfoil is performed by 5 loops that work one after the other
and operate on the nodes, cells and edges of the mesh. They work by applying a kernel
on the data item referenced by the node, edge or cell. Conceptually, the application
of a kernel to a data item is independent of the application to any other item in the same set, and can therefore
be executed in parallel.
The complexity comes from reduce operations, where some edges or cells update the same data item
(associated with the same node). In these cases care must be taken to ensure the correct update
of the data. For parallel architectures such as GPUs and multi-processor clusters this issue can be
resolved by enforcing an atomic commit scheme or by colouring the mesh partitions, so that no two
partitions update the same data item simultaneously \cite{OP2_presentation}.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.45\textwidth]{pics/coloured_mesh.png}
  \caption{An example mesh, showing data dependencies between edges that affect cell data.
\label{fig:mesh_coloured}}
\end{figure}

Consider figure \ref{fig:mesh_coloured}.
Take for example edges $\alpha=(1,4)$ and $\beta=(4,5)$. Say there is a data item $x$ associated 
with every cell and the processing of an edge increments the data items associated with it's two cells.
$\alpha$ and $\beta$ cannot execute in parallel because they are both associated with cell 0 and can therefore
end up using out of date copies of the data associated with cell 0 by the following sequence of events:
$\alpha$ reads initial $x_0$, $\beta$ reads $x_0$, $\alpha$ computes $x_{\alpha}=x_0 + 1$, $\beta$ computes $x_{\beta} = x_0 + 1$,
$\alpha$ writes back $x_{\alpha}$, $\beta$ writes back $x_{\beta}$ and the final value of $x$ turns out to be $x_{\beta} = x_0 + 1$
instead of the desired $x_0 + 2$. Some implementations work around this issue by colouring the edges, such that no two edges of the same
colour share a cell and can therefore be processed in parallel. Figure \ref{fig:mesh_coloured} shows such a colouring. Another option
would be to introduce atomic operations and/or locking, but that approach would severely limit parallelisation opportunities.

\subsection{Computational kernels and data sets}
Airfoil defines five computational kernels that iterate over the mesh, performing floating point calculations on the data sets defined over the
elements of the mesh. We shall describe them by the elements they iterate over and by the elements they read and modify. As described above,
we also define some data sets that are associated with the mesh elements. The datasets defined in Airfoil are shown in table \ref{tab:Airfoil_datasets}.

\begin{table} [H]
\begin{center}
  \begin{tabularx}{\textwidth + 30pt}{| X | X | X | X |}
    \hline                        
    Data set name & Associated with & Type/Dimension & Physical meaning \\ \hline \hline
    x & Nodes & $\mathbb{R} \times \mathbb{R}$ & Node coordinates\\ \hline
    q & Cells & $\mathbb{R} \times \mathbb{R} \times \mathbb{R} \times \mathbb{R}$ & density, momentum, energy per unit volume\\ \hline
    q\_old & Cells & $\mathbb{R} \times \mathbb{R} \times \mathbb{R} \times \mathbb{R}$ & values of q from previous iteration \\ \hline
    res & Cells &  $\mathbb{R} \times \mathbb{R} \times \mathbb{R} \times \mathbb{R}$ & residual\\ \hline
    adt & Cells &  $\mathbb{R}$ & Used for calculating area/timestep\\ \hline
    bound & Edges & $\{0, 1\}$ & Specifies whether an edge is on the boundary of the mesh\\ \hline
  \end{tabularx}
\end{center}
  \caption{Table showing the data sets and their types. In the actual implementation, we may choose to represent real numbers ($\mathbb{R}$) 
  as standard or double precision floating point numbers or as fixed point numbers (discussed later). Elements of dimension larger than one will be
  represented as arrays. The physical meaning of these sets is not important, however Airfoil is generally intereseted in computing a steady-state
  solution for the q data set.}
  \label{tab:Airfoil_datasets}
\end{table}

 The kernels are presented in table  \ref{tab:Airfoil_kernels} along with the datasets they require and modify.
\begin{table} [H]
  \begin{tabular}{| l | c | c | c |}
    \hline                        
    Kernel Name & Iterates over & Reads & Writes \\ \hline \hline
    save\_soln & Cells & q & q\_old \\ \hline
    adt\_calc & Cells & x, q & adt\\ \hline
    res\_calc & Edges & x, q, adt & res\\ \hline
    bres\_calc & (Boundary) Edges & x, q, adt, bound & res \\ \hline
    update & Cells & q\_old, adt, res & q, res \\ \hline
  \end{tabular}
  \caption{Table showing the kernels defined in airfoil and their data requirements.}
  \label{tab:Airfoil_kernels}
\end{table}
To show a more concrete example of what these kernels do, the res\_calc kernel code in the C language is presented in
Listing \ref{lst:ResCalcKernelC}. The rest of the kernels are reproduced in the appendix.

\begin{listing}[H]
  \begin{minted}[linenos]{c}
  void res_calc(float *x1, float *x2, float *q1, float *q2,
                      float *adt1, float *adt2, float *res1, float *res2) {
    float dx,dy,mu, ri, p1,vol1, p2,vol2, f;
    dx = x1[0] - x2[0];
    dy = x1[1] - x2[1];
    ri = 1.0f/q1[0];
    p1 = gm1*(q1[3]-0.5f*ri*(q1[1]*q1[1]+q1[2]*q1[2]));
    vol1 = ri*(q1[1]*dy - q1[2]*dx);
    ri = 1.0f/q2[0];
    p2 = gm1*(q2[3]-0.5f*ri*(q2[1]*q2[1]+q2[2]*q2[2]));
    vol2 = ri*(q2[1]*dy - q2[2]*dx);
    mu = 0.5f*((*adt1)+(*adt2))*eps;
    f = 0.5f*(vol1* q1[0] + vol2* q2[0] ) + mu*(q1[0]-q2[0]);
    res1[0] += f;
    res2[0] -= f;
    f = 0.5f*(vol1* q1[1] + p1*dy + vol2* q2[1] + p2*dy) + mu*(q1[1]-q2[1]);
    res1[1] += f;
    res2[1] -= f;
    f = 0.5f*(vol1* q1[2] - p1*dx + vol2* q2[2] - p2*dx) + mu*(q1[2]-q2[2]);
    res1[2] += f;
    res2[2] -= f;
    f = 0.5f*(vol1*(q1[3]+p1) + vol2*(q2[3]+p2) ) + mu*(q1[3]-q2[3]);
    res1[3] += f;
    res2[3] -= f;
  }
  \end{minted}
  \caption{
    Definition of the res\_calc kernel with reals represented as single precision floating point numbers.
    Note the type signature. The kernel requires the element of the dataset x associated with each of the two nodes
    of the edge we are currently processing and the q, adt and res elements of the two cells associted with the current edge.
    Note that the res set is updated by incrementing. The important part of this are the data requirements of the kernel
    and not the exact meaning of the arithmetic operations. The variables $gm1$ and $eps$ are global constants that do not
    need to be passed in explicitly.\label{lst:ResCalcKernelC}
  }
\end{listing}

\subsection{Indirection maps}
Having defined the data sets and the kernels, we now need to define the indirection maps that express the connectivity
of the mesh and the relationships between the elements of the mesh. Airfoil has five such maps called: edge, cell, ecell, bedge, becell.
They are presented in figure \ref{fig:Airfoil_maps}.

\begin{figure}[H]
  \centering
    \includegraphics[width=0.60\textwidth]{pics/airfoil_maps.pdf}
  \caption{Diagram showing the maps between the mesh elements The dimension of the map is shown in parentheses next to the name. 
  Thus the map \emph{edge} relating edges to nodes with dimension 2 means that for each edge, there are two nodes associated with it.
\label{fig:Airfoil_maps}}
\end{figure}

Having specified the data sets, indirection maps and kernels, the application of a kernel on an element is performed by looking up the 
mesh elements that element is associated with through the indirection maps and using those to access the data sets required by the kernel.
For example, the invokation of the res\_calc kernel defined in Listing \ref{lst:ResCalcKernelC} can be done with the following line of C:
\begin{minted}{c}
  res_calc(
          &x[2*edge[2*i]], &x[2*edge[2*i+1]], &q[4*ecell[2*i]],
          &q[4*ecell[2*i+1]], &adt[ecell[2*i]], &adt[ecell[2*i+1]], 
          &res[4*ecell[2*i]], &res[4*ecell[2*i+1]]
          );
\end{minted}
Recall that res\_calc operates on edges, and correlate the arguments to the type signature in Listing \ref{lst:ResCalcKernelC}. There
is a double level of indirection going on here. $i$ is the number of the edge we are currently processing ($i$ ranges in $[0..number\_of\_edges - 1]$).
As described in the section on mesh representation, the two nodes corresponding to the edge are stored at indices $2*i$ and $2*i+1$ of the \emph{edge} map.
For each of those nodes, res\_calc requires the corresponding element in the $x$ data set. Recall from table \ref{tab:Airfoil_datasets} that the $x$ set
has a dimension of 2. Therefore the node numbers acquired from  $edge[2*i]$ and $edge[2*i+1]$ are multiplied by 2 and used as indices
into the array $x$ to access the correct data. Similarly for the rest of the arguments.

The complete iteration step in a sequential implementation of Airfoil is shown in Listing \ref{lst:AirfoilIteration}.
 The old values of $q$ are stored in $q\_old$ and the inner loop runs twice before saving the solution again. The metric $rms$ is computed
 in each iteration that is used to measure the convergence of the solution. The variables $ncell$, $nedge$, $nbedge$ represent the number of cells, 
the number of edges and the number of boundary edges respectively.

A run of the sequential version in Listing \ref{lst:AirfoilIteration} on a mesh with 721801 nodes, 1438600 edges, 2800 boundary edges and 720000 cells
on a machine with an Intel Core i7-2600 CPU at 3.4 GHz takes about 115.6 seconds to complete 2000 iterations.
The time spent in each kernel is presented in table \ref{tab:Airfoil_timesCPU}. It is evident that the computation is dominated by the res\_calc and adt\_calc
kernels.
In this project we will be concentrating on accelerating the res\_calc kernel because it is the most computationally intensive kernel and because it has the most
complex data access patterns that make it the interesting case to study. Finding a way to accelerate res\_calc would pave the way for accelerating any similar kernel.

\begin{table} [H]
  \begin{tabular}{| c | c | c |}
    \hline                        
    Kernel Name & Time spent (seconds) & Percentage of total time (\%) \\ \hline \hline
    save\_soln & 1.84 & 1.59 \\ \hline
    adt\_calc & 51.09 & 44.19\\ \hline
    res\_calc & 53.99 & 46.70\\ \hline
    bres\_calc & 0.23 & 0.20 \\ \hline
    update & 8.44  & 7.30\\ \hline
  \end{tabular}
  \caption{Table showing the time spent in each kernel during a run of a single-threaded sequential version of Airfoil on a current CPU. The total run time is 115.6 seconds.}
  \label{tab:Airfoil_timesCPU}
\end{table}

\begin{listing}[H]
  \begin{minted}[linenos]{c}
  int niter = 1000;
  float rms = 0.0;
  for(int iter=1; iter<=niter; iter++) {
    for (int i = 0; i < ncell; ++i) {
      save_soln(&q[4*i], &qold[4*i]);
    }
    for(int k=0; k<2; k++) {

      for (int i = 0; i < ncell; ++i) {
        adt_calc(&x[2*cell[4*i]], &x[2*cell[4*i+1]],
                 &x[2*cell[4*i+2]], &x[2*cell[4*i+3]],
                 &q[4*i], &adt[i]
                );
      } 

      for (int i = 0; i < nedge; ++i) {
        res_calc(&x[2*edge[2*i]], &x[2*edge[2*i+1]],
                 &q[4*ecell[2*i]], &q[4*ecell[2*i+1]],
                 &adt[ecell[2*i]], &adt[ecell[2*i+1]],
                 &res[4*ecell[2*i]], &res[4*ecell[2*i+1]]
                );
      }
      
      for (int i = 0; i < nbedge; ++i) {
        bres_calc(&x[2*bedge[2*i]], &x[2*bedge[2*i+1]],
                  &q[4*becell[i]], &adt[becell[i]],
                  &res[4*becell[i]], &bound[i]
                 );
      }

      rms = 0.0;
      for (int i = 0; i < ncell; ++i) {
        update(&qold[4*i], &q[4*i], &res[4*i], &adt[i], &rms);
      }
    }
    rms = sqrt(rms/(float) ncell);
    if (iter%100 == 0)
      printf(" %d %10.5e \n",iter,rms);
  }
  \end{minted}
  \caption{
      The iteration structure of Airfoil.
\label{lst:AirfoilIteration}
  }
\end{listing}


\section{Hardware platform, Maxeler toolchain and the streaming model of computation}
The toolchain we use for implementing the FPGA accelerator is the one developed and maintained
 by Maxeler Technologies. It consists of the MAX3 cards that contain a Xilinx Virtex-6 chip \cite{Virtex6Spec}
and up to 48GB of DDR3 DRAM. These cards can be programmed through
 MaxCompiler\cite{MaxCompiler_whitepaper}, which provides a Java-compatible object-oriented
API to specify the dataflow graph. MaxCompiler will then schedule the graph, i.e. it will 
insert buffers that will introduce the appropriate delays in the design that will ensure
the correct values will reach the appropriate stages in the pipeline at the correct clock cycle.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.74\textwidth]{pics/max_toolchain.pdf}
  \caption{A diagram of the Maxeler toolchain. The data-flow graphs of the computational kernels
are defined using a Java API. A manager connects multiple kernels together and handles the
streaming to and from the kernels of data. These are combined by MaxCompiler and compiled
into a .max file that can then be linked to a host C/C++ or Fortran application using standard
tools (gcc, ld etc).   \label{fig:max_toolchain}}
\end{figure}

It will then produce a hardware design in VHDL that will then be further be compiled down to
a binary bitstream that configures the FPGA by the Xilinx proprietary tools. The bitstream is
then included in what is termed a \emph{maxfile} that contains various other meta-data about the 
design such as I/O stream names, named memory and register names, various runtime parameters etc.
The maxfile can be linked against a normal C/C++ application using standard tools (gcc, ld etc).
The interaction with the FPGA is performed by a low-level runtime: MaxCompilerRT and a driver
layer: MaxelerOS. A diagram of the toolchain is shown in figure~\ref{fig:max_toolchain}
\cite{MaxCompiler_whitepaper}.

Computational kernels in MaxCompiler have input streams that are pushed through a pipelined
dataflow graph and some of them are output from the kernel. Programmatically, a hardware stream
is seen as analogous to a variable in conventional programming languages. It's value potentially
changes each cycle.

\subsection{MaxCompiler example}
We present a MaxCompiler design that computes a running 3-point average of a stream of floating point values (32 bits)
in Listing~\ref{lst:MaxCompilerMovAvg}.
\clearpage
\begin{listing}[h]
	\begin{minted}[linenos]{java}
pulic class MovingAverageKernel extends Kernel {

  public MovingAverageKernel(KernelParameters parameters) {
    super(parameters);
    HWType flt = hwFloat(8,24);
    HWVar x = io.input("x", flt ) ;
    HWVar x_prev = stream.offset(x, -1);
    HWVar x_next = stream.offset(x, +1);
    HWVar cnt = control.count.simpleCounter(32, N);
    HWVar sel_nl = cnt > 0;
    HWVar sel_nh = cnt < (N-1);
    HWVar sel_m = sel_nl & sel_nh;
    HWVar prev = sel_nl ? x_prev : 0;
    HWVar next = sel_nh ? x_next : 0;
    HWVar divisor = sel_m ? 3.0 : 2.0;
    HWVar y = (prev+x+next)/divisor;
    io.output("y" , y,  flt);
  }
}
	\end{minted}
\caption{
A MaxCompiler definition of a kernel that computes a moving 3-point average with boundary conditions. Note that the arithmetic
operators as well as the ternary if operator have been overloaded for HWVar objects that represent the value of a hardware stream.
 \label{lst:MaxCompilerMovAvg}
}
\end{listing}
MaxCompiler code is written in a Java-like language called MaxJ that provides overloaded operators such as $+, - ,  *, /$ and $?:$ .
 The example in Listing \ref{lst:MaxCompilerMovAvg} creates a computational kernel that computes a stream of running 3-point
 averages,named $y$, from a stream of input values $x$. The HWVar class is the main representation of the value of a hardware
 stream at any clock cycle. HWVars always have a HWType that expresses the type of the stream (i.e. an integer, a floating point
 number, a  1-bit boolean value etc). The $stream.offset(x,-1)$ and $stream.offset(x,+1)$ expressions on lines 7 and 8 extract
 HWVars for the values of the stream on cycle in the past and one cycle in the future (note that this is internally done by creating
 implicit buffers, or FIFOs, and scheduling the pipeling accordingly). The ternary if operator $?:$ creates multiplexers in hardware
 that express choice. A Java API is provided that contains various useful design elements, such as counters (HWVars that increment
 their values in many configurable ways every cycle) that can be accessed through the control.count field.

The resulting dataflow graph can be seen in figure~\ref{fig:MovAvgGraph}
\begin{figure}[h]
  \centering
    \includegraphics[width=0.75\textwidth]{pics/MovAvgKernel.png}
  \caption{The dataflow graph resulting from the code in Listing~\ref{lst:MaxCompilerMovAvg}   \label{fig:MovAvgGraph}}
\end{figure}

Kernel designs form part of a MaxCompiler design. The user also specifies a manager that describes the streaming connections
between the kernels. A manager can be used to configure a design to stream data to and from the host through PCIe or from the
DRAM that is attached to the FPGA. In the manager design, the user will instantiate the kernels and connect them up.
Thus for the example in Listing~\ref{lst:MaxCompilerMovAvg} the manager might look like the one in
 Listing~\ref{lst:MaxCompilerMovAvgManager}.
 \clearpage
\begin{listing}[H]
	\begin{minted}[linenos]{java}
pulic class MovingAvgManager extends CustomManager {

  public MovingAvgManager(MAXBoardModel board_model,
                          boolean is_simulation, String name) {
    super(is_simulation, board_model, name);
    KernelBlock k
      = addKernel(
          new MovingAverageKernel(makeKernelParameters("MovingAverageKernel"))
                 );

    Stream x = addStreamFromHost("x");
    k.getInput("x") <== x;

    Stream y = addStreamToHost("y");
    y <== k.getOutput("y");
  }
}
  \end{minted}
\caption{
Manager specification for a MovingAverageKernel that streams the input data "x" from the host and streams the output data "y" to
the host. The $<==$ operator means connect the right hand side stream to the left hand side stream. The above code instantiates the MovingAverageKernel, creates a stream called "x" 
from the host and connects it to the input stream "x" in the kernel. Then it creates a 
stream to the host called "y" and connects to it the output stream "y" from the kernel.
 \label{lst:MaxCompilerMovAvgManager}
}
\end{listing}

After we have specified a manger, we can build the design in order to create the .max file using the following lines of code:
\begin{minted}{java}
 public class MovingAvgHWBuilder {
   public static void main(String argv[]) {

     MovingAvgManager m 
       = new MovingAvgManager(MAX3BoardModel.MAX3242A,
                              false,
                              "MovingAverage");
     m.build() ;
   }
 }
\end{minted}
This builds our design for a MAX3 card (containing a Xilinx Virtex6 FPGA) using the "MovingAverage" name for the design.

Now that we have a .max file, we can interact with the FPGA from the host code by using
the MaxCompilerRT API, an example of which is shown in Listing~\ref{lst:MovAvgHostCode}.
In order to use the FPGA we must initialise the maxfile as in line 14 and open the device (line 15). The actual
streaming to and from the FPGA is done using the max\_run vararg function (line 22) where
the arrays corresponding to the input data and the allocated space for the output data are
specified. The MaxCompilerRT runtime and the MaxelerOS drivers handle the low-level 
details of PCIe streaming and interrupts.

\begin{listing}[H]
  \begin{minted}[linenos]{c}
#include<stdlib.h>
#include<stdint.h>
#include<MaxCompilerRT.h>
#define DATA_SIZE 1024

int main(int argc, char* argv[]) {
	char* device_name = "/dev/maxeler0";
	max_maxfile_t* maxfile;
	max_device_handle_t* device;
	float *data_in, *data_out;

	maxfile = max_maxfile_init_MovingAverage();
	device = max_open_device(maxfile, device_name);
	
	data_in = (float*)malloc(DATA_SIZE * sizeof(float));
	data_out = (float*)malloc(DATA_SIZE * sizeof(float));

	for (int i = 0; i < DATA_SIZE; ++i) {
		data_in[i] = i;
	}

	max_run(device,
	        max_input("x", data_in, DATA_SIZE * sizeof(float)),
	        max_output("y", data_out, DATA_SIZE * sizeof(float)),
	        max_runfor("MovingAverageKernel", DATA_SIZE),
	        max_end());


	for (int i = 0; i < DATA_SIZE; ++i) {
		printf("data_out@%d = %f\n", i, data_out[i]);
	}

	max_close_device(device);
	max_destroy(maxfile);
	return 0;

}
  \end{minted}
\caption{
  A sample host code using the MaxCompilerRT API for the C language. 
 \label{lst:MovAvgHostCode}
}
\end{listing}

\subsection{Hardware}
The MAX3 card we use provides 48GB of DRAM that can be accessed with a maximum bandwidth of 38GB/s
and a PCIe connection to the host machine that achieves a maximum bandwidth of 2GB/s in both directions.
The Virtex6\cite{Virtex6Spec} FPGA by Xilinx used in the MAX3 card has about 4MB of fast on-board block RAM that should not
be confused with the external DRAM. The host machine can communicate with the card through the PCIe bus
using the MaxCompilerRT API. The external DRAM will be used to store the bulk of the mesh data and therefore
achieving maximum utilisation of it is be one of the focal points of this project. The top-level parts of the hardware
we are dealing with are presented in figure \ref{fig:maxHW}.

The FPGA provides a number of resources that can be used to specify a design. The Xilinx Virtex6 chip we
are using defines four such elements:
\begin{itemize}
\item{LUTs:} LookUp Tables are small elements of combinatorial logic that can be configured to implement any logical function.
\item{Flip Flops:} Stateful elements that can be used as registers to implement accumulators, pipeline stages etc.
\item{BRAMs:} Block RAMs are memory cells that are on the chip itself and can be accessed with very low latency.
\item{DSPs:} Elements custom tuned for fast multiplication.
\end{itemize}
When building an FPGA design, one must be careful to not use more resources than the chip has to offer, therefore
these numbers place a limit on the partition size we can store on the chip at any time, the number of arithmetic pipelines available etc.

\begin{figure}[H]
\begin{tikzpicture}
\draw [thick](0, 20) rectangle (5, 12);
\fill [blue!3!white] (0, 20) rectangle (5, 12);
\draw [thick](0.5, 17.5) rectangle (4.5, 13);
\fill [red!6!white](0.5, 17.5) rectangle (4.5, 13);

\node (maxcard) at (1, 19.5) {MAX card};
\node (dram) at (2.5, 18.5) [draw=black, fill=green!10!white, minimum width = 4cm] {DRAM};
\node (manager) at (2.5, 17) [draw=black, fill=orange!10!white,  minimum width = 2.7cm, minimum height=0.7cm] {Manager};
\node (kernel) at (2.5, 15) [draw=black, fill=red!10!white, minimum height=2cm,  minimum width = 2.7cm] {Kernel};
\node (fpga) at (2, 13.5) {FPGA (Virtex 6)};

\node (pcie) at (7, 17) [draw=black, fill=green!10!white, minimum height = 5cm] {PCIe};
\node (host) at (9, 17) [draw=black, fill=blue!3!white, minimum height = 5cm] {Host};

\draw [->] ($(dram.south) + (-0.2, 0)$) -- ($(manager.north) + (-0.2, 0)$);
\draw [->] ($(manager.north) + (0.2, 0)$) -- ($(dram.south) + (0.2, 0)$);
\draw [->] ($(manager.south) + (-0.2, 0)$) -- ($(kernel.north) + (-0.2, 0)$);
\draw [->] ($(kernel.north) + (0.2, 0)$) -- ($(manager.south) + (0.2, 0)$);
\draw [->] ($(manager.east) + (0, -0.1)$) -- ($(pcie.west) + (0, -0.1)$);
\draw [->] ($(pcie.west) + (0, 0.1)$) -- ($(manager.east) + (0, 0.1)$);
\draw [->] ($(pcie.east) + (0, -0.1)$) -- ($(host.west) + (0, -0.1)$);
\draw [->] ($(host.west) + (0, 0.1)$) -- ($(pcie.east) + (0, 0.1)$);
\end{tikzpicture}
\caption{Diagram of the hardware parts of a MAX card, showing the relationships between the DRAM, PCIe, the host
and the FPGA.}
\label{fig:maxHW}
\end{figure}

\subsection{Mesh Partitioning and halos}
In computing the optimal memory layout for our application, we have to partition large meshes into partitions
that fit in the block RAM of the FPGA. We use a popular and widely available set of tools called METIS developed
by George Karypis \cite{METISPaper} that uses state of the art techniques to partition meshes, graphs, hypergraphs and matrices
according to various parameters like size, edge/hyperedge cut, minimising certain metrics etc. It is a highly robust and efficient
 tool that we use through its C API. Since an iteration over a mesh element may require data associated with another element, partitions have a set of elements
called a \emph{halo region} that consists of all the elements (cells, nodes, edges) that maybe accessed from another partition.
Consider figure \ref{fig:partition}. The partitioning is shown with the red line. The four partitions share cells (shown in purple), edges (shown in red)
and nodes (along the red line). This presents a difficulty when computing an edge that uses cells in the halo region of another partition,
since we are storing a single partition at a time on the device. The method used to deal with this issue is called the
\emph{halo exchange mechanism} and it opens up a large design space, with decisions usually dependent on the hardware and communication
frameworks.

\begin{figure}[H]
  \begin{tikzpicture}


  \fill[green!10!white] (-5.0, 6.0) rectangle (0.0, 1.0);
  \fill[green!10!white] (-5.0, -5.0) rectangle (0.0, 0.0);
  \fill[green!10!white] (1.0, 1.0) rectangle (6.0, 6.0);
  \fill[green!10!white] (1.0, 0.0) rectangle (6.0, -5.0);

  \fill[blue!10!white] (-5.0, 0.0) rectangle +(11.0, 1);
  \fill[blue!10!white] (0.0, 6.0) rectangle +(1, -11.0);
  \fill[blue!10!white] (-5.0, -1.0) rectangle (6, 0);
  \fill[blue!10!white] (-1, -5) rectangle (0, 6);


  \draw (-2.5, 3.5) node {Partition 1};
  \draw (3.5, 3.5) node {Partition 2};
  \draw (3.5, -2.5) node {Partition 3};
  \draw (-2.5, -2.5) node {Partition 4};


  \draw[step=1cm] (-5.0, -5.0) grid (6.0, 6.0);
  \draw[red, very thick] (-5.0, 0.0) -- (0.0, 0.0);
  \draw[red, very thick] (0.0, 6.0) -- (0.0, 0.0);
  \draw[red, very thick] (6.0, 0.0) -- (0.0, 0.0);
  \draw[red, very thick] (0.0, -5.0) -- (0.0, 0.0);

  \draw (-0.5,2.5) node[rotate=90] {Halo region 1};
  \draw (-2.5, 0.5) node{Halo region 1};
  \draw (-2.5,-0.5) node{Halo region 4};
  \draw (0.5,2.5) node[rotate=270] {Halo region 2};

  \draw (2.5,0.5) node {Halo region 2};
  \draw (2.5, -0.5) node{Halo region 3};
  \draw (0.5, -2.5) node[rotate=90]{Halo region 3};
  \draw (-0.5,-2.5) node[rotate=270] {Halo region 4};

  \end{tikzpicture}
  \caption{A mesh partitioned into 4 partitions, shown in green. The halo regions are shown in purple. Nodes, cells and edges
  belonging to the halo region can be accessed by another partition.}
  \label{fig:partition}
\end{figure}

\subsection{Floating point vs fixed point arithmetic}
As presented in table \ref{tab:Airfoil_datasets}, the most important data sets in Airfoil (q, adt, res) consist of
real numbers. Therefore a decision must be made on the low level number representation. The most common representation
for real numbers in most modern architectures is the IEEE-754\cite{IEEEFP} floating point representation. This representation
stores the number using three fields: the sign bit, the mantissa and the exponent. The representation of a 32-bit floating point
number is shown in figure \ref{fig:FPSP}. The standard also specifies double precision floating point numbers using 64 bits:
11 bits for the exponent and 53 bits for the mantissa. In general, a floating point number with $N$ bits for the exponent
can represent a range from $\pm 1 \times 2^{-\frac{2^N}{2}-2}$ to $\pm 2 \times 2^{\frac{2^N}{2} - 1}$. For 32-bit single precision
numbers, this range is $[\pm 1\times 2^{-126}..\pm 2\times 2^{127}]$. The details of how the mantissa, the exponent and the sign bit
are used to encode a real number are presented in the IEEE specification \cite{IEEEFP}. The decoding of a floating point number involves multiplying the
mantissa by 2 raised to the power of the exponent, which is typically an expensive operation. Further details, like exponent bias
 and normalisation are not discussed here as they are tangential to this section.

\begin{figure}[H]
\begin{tikzpicture}
\fill[blue!10!white] (0, 1) rectangle (1,0);
\fill[green!10!white] (1, 1) rectangle (4.5, 0);
\fill[red!10!white] (4.5, 1) rectangle (12.5,0);

\draw (0, 1) rectangle (1,0);
\draw (1, 1) rectangle (4.5, 0);
\draw (4.5, 1) rectangle (12.5,0);
\draw (0.5, 0.5) node {+/-};
\draw (2.5, 0.5) node {8 bits exponent};
\draw (8.5, 0.5) node {23 bits mantissa};
\draw (0, 1.5) node {0};
\draw (1, 1.5) node {1};
\draw (4.5, 1.5) node {8};
\draw (12.5, 1.5) node {31};

\end{tikzpicture}
\caption{The representation of an IEEE-754 single precision floating point number. It has 8 bits for the exponent and 24 bits for the mantissa.
However, one bit of the mantissa is used to represent the sign, and is therefore unavailable to the rest of the mantissa.}
\label{fig:FPSP}
\end{figure}

Since we are working with custom hardware, we have an alternative to floating point numbers for our representation of real arithmetic.
We can store a real number as an integer and a fractional part at fixed offsets. This approach makes the arithmetic much simpler
and hence faster, but it sacrifices range and accuracy. Fixed point representation is used in applications where the range of the
numbers is predictable and not too large, or when the inputs are of limited precision. Faster in this context means fewer cycles
taken to perform an operation, which translates to pipeline stages. For example, a fixed point number with 8 integer bits and 8 fraction bits
using two's complement mode for negative numbers can repressent numbers from $-128$ up to $127 + 255/256$
 in increments of $1/256$ with each fraction bit representing $1/256$.


\section{Previous work}
Computation using unstructured meshes is widely used in many areas of engineering, not just in
fluid dynamics, and there have been many attempts to augument the computation using accelerators.
A recent trend has been to use the many cores available on Graphics Processing Units (GPUs) to
launch thousands of threads in parallel, exploiting the parallel nature of many of these problems.
Other hardware platforms include many-core architectures \cite{OP2_presentation} and large parallel clusters \cite{OP2_Cluster}.
A project close to this one is OP2 \cite{OP2_presentation}. It is a framework used to specify computations on
unstructured meshes in a hardware-agnostic way, allowing the user to concentrate on the functional specification
of the algorithm. Airfoil is one of the test programs for that project.

More relevant to this project, there have been attempts to use FPGAs to accelerate such computations.
The principles of acceleration using FPGAs are quite different compared to using GPUs or many-core
systems. In the case of FPGA acceleration, the performance advantage comes from a custom, application
specific, deeply pipelined datapath, often thousands of cycles deep that provides a throughput of one
result per cycle. This argument for FPGA acceleration is widely accepted and is the source of the speedups
achieved in all current attempts. Given this deep custop pipeline, it is a challenge for the developer
to keep the pipeline fully utilised for the maximum amount of time and the techniques used to achieve this
have been the main differentiating factors in the applications existing today.
The most common approach has been to use on-chip block RAM memory to cache small parts of the mesh and operate
on it, cache the results and write them back to main memory.

M.T. Jones and K.Ramachandran \cite{UnstructuredMeshCCM} formulate the unstructured mesh computation as a sparse matrix
problem, $Ax = y$ where $A$ is a large sparse matrix representing the mesh and $x$ is the vector
that is being approximated. Their approach uses the conjugate gradient method to 
iteratively refine the approximation of the $x$ vector. This involves, most importantly, a 
multiplication of the sparse matrix $A$ with the vector $x$ which forms the bulk of the computation 
and a subsequent refinement of the mesh and reconstruction of the spares matrix. They formulate the problem
as a sparse matrix-vector multiplication, whereas we are interested in iterating computational kernels
over the mesh. Furthermore, they are concerned with mesh adaptation and the reconstruction of the sparse
matrix in each iteration. We assume a static mesh specified at the beginning of the program.

Morishita et al. \cite{MemoryHierarchy} examine the acceleration of CFD applications and in
 particular the use of on-chip block RAM resources to buffer the data in order to keep the
 arithmetic pipeline as full as possible. This is a more similar approach. However, their approach
applies a constant stencil to a grid in 3D and tries to cache points in the grid that will be 
accessed in the next iteration, thus elimintating redundant accesses to the external memory. This
caching/buffering is made possible by the fact that the stencil is of constant shape and thus the
memory accesses can be predicted. In our application we have a 2D mesh that does not exhibit this
property.

Sanchez-Roman et al. \cite{SpanishFPGAAirfoil} present the acceleration of an airfoil-like
 unstructured mesh computation using FPGAs. Their solution uses two FPGAs on a single chip that
 perform different calculations and they identify the need to reason about computation and data access
separately. They mention the need to partition larger meshes but they do not
 discuss techniques for partitioning or the issues arising from data dependencies across
partitions. They mention the degradation in performance arising from the unstructured memory access
patterns causing cache misses. We present a technique to reorganise the mesh so as to facilitate
more well-behaved memory accesses, allowing us to stream data to the datapath efficiently.

In another attempt, Sanchez-Roman et al. \cite{SpanishFPGAAirfoil2} recognise the update dependency that
occurs during reduction operations, similar to the ones in our res\_calc kernel and work around it by
adding an accumulator that correctly updates the required data sets. However, their design is used on
comparatively small meshes of a maximum of 8000 nodes. Thus all the data can fit into the on-board memory
of the FPGA, eliminating the need to consider partitioning issues. The applications we are concerned with
usually have meshes of the order of $10^6$ edges, which will definitely not fit on the on-chip block RAMs
any time soon, thus presenting the need for partitioning.

The existing work gives us many hints towards design decisions. While it was tempting to use fixed point
arithmetic for the calculations, Sanchez-Roman et al. \cite{SpanishFPGAAirfoil} hint at the difficulty
in porting application from the host side to fixed point and also mention great difficulties that arise
in the debugging and verification of the calculations.
Furthermore, Durbano et al. \cite{ElectromagneticsFPGA} find that the error of fixed-point arithmetic 
accumulates with each iteration in finite difference applications such as the one in Airfoil. They
explore the possibility of acceleration of electromagnetics calculations using the Finite Difference
Time Domain method (FTDT) which has some similarities to ours from a computational point of view.
These factors lead us to discard fixed point representation of real numbers in our solution.
All authors mention the relative difficulty of developing FPGA-based systems compared to normal CPU
implementations, citing the different mindset required and the inherently more low level reasoning
about hardware, architecture and algorithms that must be done to extract the required performance characteristics.

All attempts recognise the need to tame the unstructured memory accesses that arise, and all of them
use the on-chip block RAM resources to cache or buffer data before feeding it to the arithmetic pipeline,
relying on complex memory access address generators to handle memory accesses. Our approach will also
use block RAMs to store parts of the mesh, but we will remove the need for complex memory access patterns
by reordering, partitioning and laying out the mesh data in a way that facilitates large, contiguous bursts
of streaming. To do that we add a mesh preprocessing stage on the host side that partitions and reorganises
the layout in memory of the data.

Some exploration has been done in adapting meshes for particular memory architectures or reordering computations
to optimise memory accesses.
White B. et al. \cite{MeshIntensityROSE} have explored techniques for reordering computations on CPUs to
facilitate efficient memory access, but not from a streaming perspective.
There has been some work done in storing meshes for efficient access\cite{MeshCache}, but it has been
focused on block-based CPU and GPU caches. We explore issues that are associated with laying out mesh data
for optimal DRAM bandwidth utilisation on FPGAs.

The differentiating factors of this project from existing work are:
\begin{itemize}
\item{We explore the architectural design space for the accelerator in conjunction with mesh 
reordering techniques custom-fitted for the chosen architecture.}
\item{Our design aims to work for large meshes, in the order of $10^5$ - $10^6$ nodes.}
\item{We attempt to keep the design of the hardware accelerator as simple as possible,
without complex memory access patterns and relying on host-side preprocessing to figure out
the optimal data layout. We are focused on maximising DRAM bandwidth utilisation.}
\end{itemize}

\chapter{Design and Modelling}

In this section we discuss the design of the hardware accelerated version of the res\_calc kernel
from Airfoil. We present the architecture for the FPGA-based accelerator and develop a formal performance
model that is used to justify the viability of the architecture. From that point we decide on an optimal
data layout that will lead to an implementation of the mesh preprocessing on the host.

\section{DRAM and mesh storage}
The MAX3 cards we have available have a large DRAM memory attached to them, and it is a natural
candidate for storing mesh data. The computational kernel on the FPGA can access this memory
at a maximum bandwidth of 38GB/s. This bandwidth, however, is only achievable when the memory
is accessed in large bursts of contiguous addresses. Random access to this memory, while possible,
is very inefficient and wasteful (because the DRAM controller will still read a large chunk of data,
but return only the small fraction requested). Because of the representation of the unstructured mesh
through indirection maps, processing an edge in res\_calc requires a lookup in the \emph{edge} and \emph{ecell}
maps, and then a lookup into the $x$, $q$ and $adt$ data sets. If we perform such a two-level dereferencing
procedure on the DRAM, the performance is expected to degrade to a point where it is not worth considering.
The on-chip block RAMs, on the other hand, are designed to be accessed randomly and do not degrade
in performance when accessed so. Thus, we want to push the indirection down to the block RAMs. In other words,
mesh connectivity information must not affect the DRAM access pattern and be entirely contained
in the data layout and addressing of the block RAMs. This is the argument for storing
one mesh partitions in the block RAMs, using connectivity information to access it randomly without penalty
and feeding the result into an arithmetic pipeline that will perform the floating point calculations.
Under this arrangement, the edges are then represented as a vector of addresses into the node and cell RAMs.


\section{Result accumulation and storage}
Remember that res\_calc performs an incrementing operation (also known as \emph{reduction} with addition) on the
$res$ data set for the cells that each edge accesses. Therefore each edge computes only part of the final value
of $res$ of its cells. Therefore the arithmetic pipeline, upon processing each edge will produce increments
that must be correctly summed up for each cell before that cell is output. Thus the result of the arithmetic
pipeline must be added to the current value of res for the relevant cells. We use more block RAMs to store the intermediate
$res$ results, since the access pattern to $res$ is the same as the access pattern for the cells.

The architecture diagram of that description is shown in figure \ref{fig:Architecture1st}. The node and cell data for the
current partition are streamed in from the DRAM and stored in the block RAMs. They are then read in a pattern described
by the connectivity between edges, nodes and cells and fed into the pipeline that produces the $res$ increments for two
cells that must then be added to the currenty values of $res$ for those cells. Thus we have an accumulator node between
the block RAMs for $res$ and the arithmetic pipeline.

The steps required to process a partition become:
\begin{enumerate}
\item{Read in node and cell data from DRAM and store it locally.}
\item{Process the partition data and store the $res$ data set locally.}
\item{Write out the resulting $res$ set back to DRAM.}
\end{enumerate}

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\node (dram) at (5, 20) [rectangle, draw=black, fill=blue!10!white, minimum width=3cm] {DRAM (Node/Cell data)};
\node (edges) at (10, 20) [rectangle, draw=black, fill=blue!10!white, minimum width=3cm] {DRAM (Edges/addresses)};
\node (x) at (3,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Node data: $x$};
\draw [->, very thick] (dram.south) to [out=270, in=90] (x.north);
\node (cell) at (7,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Cell data: $q$, $adt$};
\draw [->, very thick] (dram.south) to [out=270, in=90] (cell.north);
\node (arith) at (5, 15) [rectangle split, rectangle split parts=4, draw=black, fill=green!10!white, minimum height=4] 
{Arithmetic pipeline };
\draw [->,very thick] (x.south) to [out=270, in=90] ($(arith.north) + (-0.5, 0)$);
\draw [->,very thick] (cell.south) to [out=270, in=90] ($(arith.north) + (0.5, 0)$);
\node (add) at (5, 12) [circle, draw=black, fill=green!10!white] {+};
\node (res) at (5, 10) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white] {Block RAM \nodepart{second} $res$};
\draw [->, dashed, thick] (edges.south) -- (x.east);
\draw [->, dashed, thick] (edges.south) -- (cell.east);
\draw [->, dashed, thick] (edges.south) -- (res.east);

\draw [->,very thick] (res.south) -- +(0,-0.5) -- ++(-3.5, -0.5) -- ++(0, +11) -- (dram.west);
\draw [->,very thick] (add.south) -- (res.north);
\draw [->,very thick] (arith.south) -- (add.north);
\draw [->,very thick] (res.east) -- +(0.5, 0) -- +(0.5, 2) -- (add.east);
\end{tikzpicture}
\end{center}
\caption{Simplified architecture diagram of the accelerator showing the block RAMs storing the node and cell data, the arithmetic pipeline, the result block RAMs
and the accumulator. The connectivity information is used to address the block RAMs.}
\label{fig:Architecture1st}
\end{figure}

\section{Halo exchange mechanism}
In Airfoil every edge references two nodes and two cells. Those nodes and cells may be contained in the halo region of
an adjacent partition. We have to devise a mechanism to acquire the required halo data.
Since the mesh connectivity is constant, we can pre-compute the neighbours and the halo regions
on the host. The difficulty arises from the reduction operation. A cell that can be accessed from two or more
partitions needs to add up the contributions of all its edges, and the four edges that
typically reference a cell will not necessarily be in the same partition. We are faced with the problem
of updating a cell from two or more partitons.
Since we perform the initial partitioning, we can identify these halo cells and store them on the host, not on
the DRAM of the card. When processing a partition, we will send these halo cells and nodes to the FPGA via PCIe.
The accelerator will then have all the data it needs to process all its edges, however the $res$ results that
it computes for the halo cells will be only partial results that need to be combined with the results
of the other partitions that access those cells. This addition of partial results will be performed on the host.
We add some logic to choose the whether to read the cell and node data from the halo RAMs or the normal RAMs to obtain
an architecture shown in figure \ref{fig:ArchitecturePCIe}. This approach to halo exchange is similar to the
ghost cell mechanism \cite{GhostCellPaper}.
Thus the stages for processing a partition become:
\begin{enumerate}
\item{Read in node and cell data from DRAM. Read in halo node and cell data from PCIe.}
\item{Process the partition data and store the $res$ data set locally. The $res$ data for the halo cells
is a partial contribution of the final value.}
\item{Write the resulting $res$ set back to DRAM. Send the partial results for halo cells back to the host through PCIe.}
\item{Once all the partitions are processed, the host adds up the contributions to the halo cells from all partitions.}
\end{enumerate}

Remember that the PCIe bandwidth is much lower than that of the DRAM (about 10 times lower), so if the halo region
of a partition constitutes a large enough percentage of the total size of the partition, the PCIe transfer becomes
dominant and the DRAM will end up being poorly utilised because the kernel will be waiting on the host transfer.
This is a factor to keep in mind when choosing partition sizes and partitioning techniques.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
[input/.style={rectangle, draw=black, fill=blue!10!white, minimum width=3cm, minimum height=1cm}]

\node (dram) at (5, 23) [input] {Node/Cell data};
\node (edges) at (14.5, 12) [input] {Edges (addresses)};
\node (edgestop) at (9, 23) [input] {Edges (addresses)};

\node (PCIe) at (13, 23) [input] {Halo data (PCIe)};


\node (x) at (3,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Node data: $x$};

\draw [->, very thick] (dram.south) to [out=270, in=90] (x.north);
\node (cell) at (7,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Cell data: $q$, $adt$};
\draw [->, very thick] (dram.south) to [out=270, in=90] (cell.north);

\node (hcell) at (15,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Halo Cell data: $q$, $adt$};
\node (hx) at (11,18) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM \nodepart{second} Halo Node data: $x$};
\draw [->, very thick] (PCIe.south) to[out=270, in=90] (hcell.north);
\draw [->, very thick] (PCIe.south) to [out=270, in=90] (hx.north);
\draw [->, dashed, thick] (edgestop.south) to[out=270, in=135] ($(hcell.north) + (-0.5, 0)$);
\draw [->, dashed, thick] (edgestop.south) to [out=270, in=135] ($(hx.north) + (-0.5, 0)$);
\draw [->, dashed, thick] (edgestop.south) to [out=270, in=45] ($(cell.north) + (0.5, 0)$);
\draw [->, dashed, thick] (edgestop.south) to [out=270, in=45] ($(x.north) + (0.5, 0)$);

\node (arith) at (9, 12) [rectangle split, rectangle split parts=4, draw=black, fill=green!10!white, minimum height=4] 
{Arithmetic pipeline };

\node (sel) at (9,14) [rectangle, draw=black, fill=orange!10!white] {RAM selector};

\draw [->,very thick] (x.south) to [out=270, in=135] ($(sel.north) + (-0.5, 0)$);
\draw [->,very thick] (cell.south) to [out=270, in=90] ($(sel.north) + (-0.25, 0)$);
\draw [->,very thick] (hx.south) to [out=270, in=90] ($(sel.north) + (0.25, 0)$);
\draw [->,very thick] (hcell.south) to [out=270, in=45] ($(sel.north) + (0.5, 0)$);
\draw [->, very thick] (sel.south) -- (arith.north);

\node (add) at (9, 9) [circle, draw=black, fill=green!10!white] {+};
\node (res) at (6, 6) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white] {Block RAM \nodepart{second} $res$};
\node (hres) at (12, 6) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white] {Block RAM \nodepart{second} Halo $res$};

\node (rsel) at (9, 7.5) [rectangle, draw=black, fill=orange!10!white] {RAM selector};
\draw [->, very thick] (res.east) to [out=0, in=270] ($(rsel.south) + (-0.5, 0)$);
\draw [->, very thick] (hres.west) to [out=180, in=270] ($(rsel.south) + (0.5, 0)$); 
% \draw [->, very thick] (rsel.east) to [out=0, in=90] (hres.north);
% \draw [->, very thick] (rsel.west) to [out=180, in=90] (res.north); 

\draw [->, very thick] ($(rsel.north)$) to (add.south);
\draw [->, dashed, thick] (rsel.east) to[out=0, in=90] ($(hres.north) + (-1, 0)$);
\draw [->, dashed, thick] (rsel.west) to[out=180, in=90] ($(res.north) + (1, 0)$);

% \draw [->, very thick] (add.east) to [out=0, in=45] ($(rsel.north) + (0.5cm,0)$);
\draw [->, very thick] (add.east) to [out=0, in=90] (hres.north);
\draw [->, very thick] (add.west) to [out=180, in=90] (res.north);

\draw [->, dashed, thick] (edges.west) to [out=135, in=0] (sel.east);
\draw [->, dashed, thick] (edges.west) to [out=225, in=45] ($(rsel.north) + (1, 0)$);
% \draw [->, dashed, thick] (edges.west) -- (x.east);
% \draw [->, dashed, thick] (edges.west) -- (cell.east);
% \draw [->, dashed, thick] (edges.west) -- (res.east);

\draw [->,very thick] (res.south) -- +(0,-0.5) -- ++(-4.5, -0.5) -- ++(0, +18) -- (dram.west);
\draw [->,very thick] (hres.south) -- +(0,-0.5) -- ++(5.5, -0.5) -- ++(0, +18.05) -- (PCIe.east);

% \draw [->,very thick] (add.south) -- (res.north);
\draw [->,very thick] (arith.south) -- (add.north);
% \draw [->,very thick] (res.east) -- +(0.5, 0) -- +(0.5, 2) -- (add.east);
\end{tikzpicture}
\caption{Architecture diagram of the accelerator with the PCIe halo exchange mechanism. The RAM selectors will select
which RAM to read the cell and node data from based on the edge information. They are also used to pick the RAM to write the
results back to. The dashed lines represent addresses that are used to access the RAMs and to determine which RAMs to access.}
\label{fig:ArchitecturePCIe}
\end{center}
\end{figure}

\section{Two-level partitioning}
The astute reader will notice that in order to process the partition, we first need to stream in the entire partition,
process it fully and write it back. During the processing phase we are not streaming anything in or out of the kernel,
leaving the DRAM unutilised, thus wasting bandwidth. To mitigate this, we introduce a second level of partitioning on
the mesh. Each partition will be split into two \emph{micro-partitions}($\mu$partitions). The idea is that as soon as we finish reading in 
the first mircro-partition, we can immediately start processing it while reading in the second micro-partition. Then, when
the second mircro-partition has finished streaming in and the processing has finished for the first one, we can write out the results
of the first one while processing the second. This allows us to achieve an overlap of data transfer and computation 
in a scheme reminiscent of a simple processor pipeline, where the fetch, computation and commit stages overlap to increase the utilisation of the relevant units.
As shown in figure \ref{fig:upartition}, the two micro-partitions will invariably share some elements in a small region we 
call the \emph{intra-partition halo}(IPH). Care must be taken to not write out the results of the intra-partition halo or overwrite
the data in it before both mirco-partitions have finished processing. 
To avoid confusion we call the top-level partitions macro-partitions.
This approach gives us the following phases in the accelerator:
\begin{enumerate}
\item{Read in data for first micro-partition plus the intra-partition halo.
 If this is not the first macro-partition, write out the non-halo data for the second micro-partition and the intra-partition halo.}
\item{Process first micro-partition, read in the non-IPH data for second micro-partition.}
\item{Process second micro-partition, write out the non-IPH data for the first micro-partition.}
\end{enumerate}
Note that "read in" and "write out" include both the DRAM and PCIe halo transfers. This approach is expected to greatly improve
DRAM utilisation at the expense of slightly more complex control logic. This imposes a constraint on the mesh layout in the DRAM and
on the host machine. The first micro-partition should be stored before the intra-partition halo and the second micro-partition last.

\begin{figure}[H]
  \begin{tikzpicture}
\fill[green!10!white] (-5, 0) rectangle (7,6);
\fill[red!10!white] (-5, 6) rectangle (7,5);
\fill[red!10!white] (-5, 0) rectangle (-4,5);
\fill[red!10!white] (-5, 0) rectangle (7,1);
\fill[red!10!white] (6, 0) rectangle (7,6);
\fill[purple!50!white] (0, 0) rectangle (1,6);
\fill[purple!50!white] (1, 0) rectangle (2,6);

\draw (-1.9, 3.5) node {$\mu$partition~~ 1};
\draw (4.1, 3.5) node {$\mu$partition~~ 2};
\draw[red, very thick] (-5, 0) rectangle (7,6);
\node (iph1) at(0.5, 3) [rotate=90] {\emph{IPH}};
\node (iph2) at(1.5, 3) [rotate=270] {\emph{IPH}};

\draw [red, very thick, dashed] (1,0) -- (1,6);
\draw[step=1cm] (-5, 0) grid (7, 6);

  \end{tikzpicture}
  \caption{Partitioning of a macro-partition into two micro-partitions. This introduces a new
intra-partition halo region, shown here in crimson.}
  \label{fig:upartition}
\end{figure}

The inputs, outputs and RAMs of the kernel can be controlled through enable signals that predicate
their function on some boolean condition that we can define. This gives us a straightforward
way to control when the kernel reads, processes or writes data. We can define a state machine
with internal counters that can be used to keep track of the progress of each phase and signal
the I/O units when data needs to be read in or written out. It can also be used to control
the block RAMs, specifying when to commit the data found on their input ports. For this, the 
state machine will need to know the sizes of the micro-partitions, the intra-partition halo and 
the external halo. This can be added to the design as a separate stream that contains vectors
of integers that represent the required sizes. Compared to the sizes of the partitions, the size
of the size vector is negligible and therefore does not impact the performance of the memory system.
Finally, we arrive at the architecture shown in figure \ref{fig:ArchitectureSM}. 
The interleaving of I/O and processing gives rise to a pipeline-like execution pattern, shown in
figure \ref{fig:partPipeline}. Notice how at every stage there is I/O activity that keeps the DRAM
and PCIe streams busy.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[input/.style={rectangle, draw=black, fill=blue!10!white, minimum width=3cm, minimum height=1cm}]
\node (dram) at (5, 20) [input] {Node/cell/halo data};
\node (edges) at (10, 20) [input] {Edges (addresses)};

\node (sizes) at (15, 20) [input] {Size vectors};
\node (SM) at (15, 17) [circle, draw=black, fill=orange!10!white]{SM};
\node (knot) at (12, 18.5) {};

\node (read) at (5, 17) [diamond, draw=black, fill=orange!10!white] {read?};
\node (process) at (10, 17) [diamond, draw=black, fill=orange!10!white] {process?};
\node (write) at (2, 17) [diamond, draw=black, fill=orange!10!white] {write?};

\node (storage) at (5,15) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white, minimum width=1cm] {Block RAM storage\nodepart{second} Nodes, Cells, Halos};
\draw [->, very thick] (dram.south) to [out=270, in=90] (read.north);
\draw [->, very thick] (read.south) to [out=270, in=90] (storage.north);
\draw [->, dashed, thick] (process.south) to [out=270, in=0] (storage.east);
\draw [->, very thick] (edges.south) to (process.north);
\draw [->, very thick] (sizes.south) to (SM.north);

\draw [red] (SM) to[out=180, in=0] (knot.east);
\draw [->, red] (knot.east) to [out=180, in=0] (process.east);
% \draw [->, red] (knot.east) to [out=180, in=0] (read.east);
\node (knot2) at (7, 18.5) {};
\draw [red] (knot.east) to [out=180, in=0] (knot2.east);
\draw [->, red] (knot2.east) to [out=180, in=0] (read.east);

\node (knot3) at (4,18.5) {};
\draw [red] (knot2.east) to [out=180, in=0] (knot3.east);
\draw [->, red] (knot3.east) to [out=180, in=0] (write.east);


\node (arith) at (5, 12) [rectangle split, rectangle split parts=4, draw=black, fill=green!10!white, minimum height=4] 
{Arithmetic pipeline };
\draw [->,very thick] (storage.south) to [out=270, in=90] (arith.north);
\node (add) at (5, 9) [circle, draw=black, fill=green!10!white] {+};
\node (res) at (5, 7) [rectangle split, rectangle split parts=2, draw=black, fill=red!10!white] {Block RAM \nodepart{second} $res$};
\draw [->, dashed, thick] (process.south) to [out=270, in=90] ($(res.north) + (1,0)$);

\draw [->, red] (SM.south) to [out=270, in=0] ($(res.east) + (0,-0.25)$);
\draw [->, red] (SM.south) to [out=270, in=0] ($(storage.east) + (0,-0.25)$);

% \draw [->, dashed, thick] (edges.south) -- (storage.east);
% \draw [->, dashed, thick] (edges.south) -- (res.east);


\draw [->,very thick] (res.south) -- +(0,-0.5) -- ++(-3, -0.5) -- (write.south);
\draw [->,very thick] (write.north) -- (2, 20) -- (dram.west);
\draw [->,very thick] (add.south) -- (res.north);
\draw [->,very thick] (arith.south) -- (add.north);
\draw [->,very thick] (res.east) -- +(0.5, 0) -- +(0.5, 2) -- (add.east);
\end{tikzpicture}
\end{center}
\caption{Architecture diagram showing the addition of a state machine (node SM) that controls the I/O and the processing.
The red wires represent the boolean enable signals. The halo and normal RAMs as well as the RAM selectors
are shown in merged blocks for brevity.}
\label{fig:ArchitectureSM}
\end{figure}

\begin{figure}
\begin{tikzpicture}[scale=0.8]
\newcommand{\firstiph}[1] {
  \fill [green!10!white] #1 rectangle +(3, -2);
  \fill [red!10!white] ($#1 + (0,-1.5)$) rectangle ($#1 + (3, -2)$);
  \draw [thick] #1 rectangle +(3, -2);
  \draw [thick] ($#1 + (0,-1.5)$) rectangle ($#1 + (3, -2)$);
  \draw [thick] #1 -- +(0, -2.4);
  \draw [thick] ($#1 + (3, 0)$) -- +(0, -2.4);
  \node at ($#1 + (1.5, -1)$) {$\mu$partition 1};
  \node at ($#1 + (1.5, -1.7)$) {IPH};
}
\newcommand{\second}[1] {
  \fill [green!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);

  \node at ($#1 + (1.5, -1.5)$) {$\mu$partition 2};
  \draw [thick] #1 -- ++(0, -2.4) -- ++(3, 0) -- ++(0, 2.4);
}
\newcommand{\secondiph}[1] {
  \fill [green!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \fill [red!10!white] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -0.9)$);
  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -0.9)$);

  \draw [thick] ($#1 + (0,-0.4)$) rectangle ($#1 + (3, -2.4)$);
  \node at ($#1 + (1.5, -0.65)$) {IPH};
  \node at ($#1 + (1.5, -1.5)$) {$\mu$partition 2};
  \draw [thick] #1 -- ++(0, -2.4) -- ++(3, 0) -- ++(0, 2.4);
}
\newcommand{\first}[1] {
  \fill [green!10!white] #1 rectangle ($#1 + (3, -2)$);
  \draw[thick] #1 rectangle ($#1 + (3, -2)$);
  \draw [thick] ($#1 + (0, -2.4)$) -- #1 -- ($#1 + (3, 0)$) -- ($#1 +(3, -2.4)$);
  \node at ($#1 + (1.5, -1)$) {$\mu$partition 1};

}

\draw (2.5, 21) node {\emph{READ}};
\draw (7.5, 21) node {\emph{PROCESS}};
\draw (12.5, 21) node {\emph{WRITE}};
\draw [very thick] (-2.5, 20.5) -- (15, 20.5);

\draw [dashed, very thick] (5, 21.5) -- (5, 0);
\draw [dashed, very thick] (10, 21.5) -- (10, 0);
\secondiph{(11, 20)};
\firstiph{(1,20)};
\second{(1, 17)};
\firstiph{(6,17)};
\secondiph{(6, 14)};
\first{(11, 14)};
\firstiph{(1,11)};
\secondiph{(11, 11)};
\second{(1, 8)};
\firstiph{(6, 8)};
\secondiph{(6, 5)};
\first{(11, 5)};
\firstiph{(1, 2)};
\secondiph{(11, 2)};
\draw [dashed] (-2.5, 17.2) -- (15, 17.2);
\draw [dashed] (-2.5, 14.2) -- (15, 14.2);
\draw [dashed] (-2.5, 11.2) -- (15, 11.2);
\draw [dashed] (-2.5, 8.2) -- (15, 8.2);
\draw [dashed] (-2.5, 5.2) -- (15, 5.2);
\draw [dashed] (-2.5, 2.2) -- (15, 2.2);
\draw [very thick] (0, 21.5) -- (0, 0);
\draw (-1.2, 18.5) node {\emph{PHASE 1}};
\draw (-1.2, 15.5) node {\emph{PHASE 2}};
\draw (-1.2, 12.5) node {\emph{PHASE 3}};
\draw (-1.2, 9.5) node {\emph{PHASE 1}};
\draw (-1.2, 6.5) node {\emph{PHASE 2}};
\draw (-1.2, 3.5) node {\emph{PHASE 3}};
\draw (-1.2, 0.5) node {\emph{PHASE 1}};

\draw [->,densely dashed, very thick] (-2.9, 21) -- (-2.9, 0);
\draw (-3.2, 10.5) node[rotate=90]{\emph{TIME}};


\newcommand{\redpart}[1]{
\draw [red, very thick, rounded corners] #1 -- ++(3.75, 0) -- ++(0, -3.15) -- ++(5.6, 0) -- ++(0, -3) -- ++(4.4,0) -- ++(0, -5.7)
-- ++(-4, 0) -- ++(0, 3) -- ++(-4.5, 0) -- ++(0, 3) -- ($#1 + (0, -5.9)$) -- cycle;
}

\redpart{(0.5, 20.25)};
\redpart{(0.5, 11.25)};

\draw [red, very thick, rounded corners] (0.2, 2.1) -- ++(4, 0) -- ++(0, -2.6);
\draw [red, very thick, rounded corners] (10.5, 20.25) -- ++(0, -2.9) -- ++(4, 0);



% \draw [->, very thick, dashed] (-3.5, 21) -- (-3.5, -1);
% \draw (-3.5, 10) node {\emph{TIME}};
\end{tikzpicture}
\caption{Diagram showing the overlapping of execution and I/O thanks to the two-level partitioning scheme.
Note that both micro-partitions need the intra-partition halo (IPH) in order to be processed, so the IPH can only
be written out together with the second micro-partition after both micro-partitions ($\mu$partitions) have been processed. The red boxes
represent the progress of a single (macro)partition through the accelerator phases.}
\label{fig:partPipeline}
\end{figure}
\section{The case for a custom streaming pipeline}
The floating point calculations will be performed by the arithmetic pipeline, custom designed for the res\_calc
kernel. In a general purpose core, like on a CPU and to a somewhat lesser extent a GPU, the floating point calculations
will be performed one after another, writing intermediate values to registers and/or cache. The calculations are expressed
as a sequence of instructions. In a custom streaming datapath we specify a dataflow graph that the $x$, $q$ and $adt$ vectors
are streamed through, and the $res$ increments come out of the bottom. The advantage of this approach is that we can add 
registers at every stage of every calculation to create a deep pipeline with high throughput. Pipelining is a well-known
processor design technique for increasing functional unit utilisation and throughput. On conventional processors it is used
with some care, avoiding very deep pipelines, because the general purpose workload these CPUs are designed for may include arbitrary sequences of instructions
that can potentially create various \emph{pipeline hazards} (such as invalidation of an instruction already in a pipeline because a previous branch instruction
was taken, a load memory instruction waiting on a write memory instruction etc.). Adding more pipeline stages may increases throughput (results per clock cycle)
 of instructions, but it also increases their latency (time for a particular instruction to complete) because of the extra registers that are added to store results
between the pipeline stages. A pipeline hazard is usually dealt with by stalling the pipeline (waiting for an instruction to complete execution) or flushing it
to remove invalid instructions. These measures introduce a performance penalty that is proportional to the depth of the pipeline \cite{Pipeline}.

These drawbacks are not applicable to our approach, because we are creating a custom datapath for a known custom workload that will perform specific
floating point operations to data that is well-formed for this particular purpose. Since we know beforehand
the calculations that will be performed and in what order we can safely pipeline the design as much as possible without worrying about data or control hazards.
 With that done, the only other concern becomes the task of keeping it occupied for as long as possible as discussed in the sections above.

An important and perhaps counter-intuitive prediction we can make is that because of the extreme pipelining the throughput of the architecture during the processing
phase will remain at one result per cycle, regardless of the actual computational workload. As we increase the computational complexity of the kernel, the pipeline
gets deeper and therefore takes more cycles to fill in the beginning and flush at the end, but during the time when it's filled (which is most of the time if the
architecture works properly and supplies inputs continuously) it produces one result per clock cycle. This gives us an intuition of why this approach to acceleration
is a good idea. Assuming large enough data sets, as in Airfoil, the time to fill and flush the pipeline will be amortised by the time spent executing. Compare this
observation with the execution of Airfoil on a CPU or a GPU. On those architectures the arithmetic execution time is expected to increase proportionally to the number of floating
point calculations performed. Using a custom streaming datapath, the increase in floating point calculations translates into more resources being used (LUTs, Flip Flops and
DSPs), but does not imply a corresponding increase in execution time. This means that a custom, deeply pipelined architecture must win in the asymptotic case
against any general purpose architecture as the number of arithmetic operations increases. Of course, in practice, the complexity of the arithmetic pipeline will be
limited by the amount of resources available on the chip.

Figure \ref{fig:StreamVsCPU} shows the difference in the custom streaming approach as opposed to a conventional CPU. Note the absence of a fetch/decode unit in the
custom approach, since we are not dealing with instructions. The arithmetic pipeline is designed to implement a particular mathematical function, while on a 
generic CPU, the Arithmetic and Logic Unit (ALU) can handle arbitrary sequences of arithmetic operations, at the expense of requiring a separate fetch/decode
unit as well as a register file to store intermediate results. Notice how the \verb!add r1, r2! instruction cannot execute until the previous instruction
\verb!mul r1, r1! has commited its result to register \verb!r1!, creating a potential data hazard that may stall any pipelines present in the functional units
of the ALU. In the streaming approach on the left, each of the functional
units is pipelined to achieve high throughput. The $y$ stream must be delayed by a FIFO queue before being sent to the adder to compensate for the cycles taken
to produce the result from the multiplication unit. Similarly, the output of the square root unit must be delayed/buffered before entering the subtraction node.
This way, we can push new values for $x$, $y$ and $z$ into the pipeline every cycle and after it has been filled, we start receiving one value for $r$ every cycle.
Notice, also, that the $x^2$ and $\sqrt{z}$ functions are computed in parallel, since they operate on different data items.
Compare this to the CPU approach on the right, where the computation of a single value for $r$ takes 8 instructions. It is evident that even with sophisticated
processor design techniques like out-of-order execution, value forwarding and instruction reordering by the compiler, it is unlikely that the CPU will be able to produce and sustain a throughput of
one value per clock cycle since the \verb!load! instructions alone will probably take at least one cycle to fetch the data from the cache or, even worse,
the main memory in the event of a cache miss. This should be especially evident on large homogeneous workloads where the time to fill and flush the custom pipeline on the left becomes 
negligible compared to the time it remains filled, providing maximal throughput, while the CPU case will have to deal with more cache misses that introduce huge performance
penalties (in the 1000s of clock cycles) due to the fact that the large data sets will simply not fit into the cache. Of course, the custom streaming approach also demands that
data be fed into the pipeline at every cycle, thus requiring additional effort by the developer to format the data accordingly as discussed in previous
sections.

A potential GPU implementation will also suffer from these problems, albeit to a lesser degree. While a GPU has hundreds of cores and arithmetic units available to run
hundreds or thousands of threads, they are still general purpose and still suffer from the need to decode instructions and access a register file and a cache/memory hierarchy.
Adding more complex arithmetic operations will still increase the time to produce a result in a linear fashion, while in a custom streaming datapath the extra
work can either be done in parallel by adding extra function nodes and streams or by adding more nodes to the pipeline, increasing the time to fill/flush it,
but maintaining the throughput.

The arithmetic pipeline we design for the res\_calc kernel will produce increments that must be added to the previous values of $res$ by the accumulator,
as discussed earlier. We choose to use single precision IEEE-754 floating point numbers for the arithmetic because previous work by
Sanchez-Roman et al. \cite{SpanishFPGAAirfoil} hints at the error accumulation that arises from repeated iterations of fixed-point calculations
as well as the implementation difficulties in encoding the data sets to and from fixed-point representation due to the fact that most host CPU
architectures do not have native fixed-point data types.
Trial runs of the serial version of Airfoil on a CPU have shown the iteration structure to converge when using single precision floating point numbers
so we do not use double precision arithmetic, since this is a proof of concept project.

Note that the res\_calc arithmetic pipeline has too many nodes to be meaningfully reproduced in a diagram here, but it is constructed using the principles
discussed in this section.

\begin{figure}[H]
\begin{tikzpicture}
\node (x) at (0, 20) {$x$};
\node (y) at (2, 20) {$y$};
\node (z) at (4, 20) {$z$};

\node (times) at (0, 18)[circle, draw=black, fill=red!10!white] {$*$};
\node (fifo) at (2, 18) [rectangle split, rectangle split parts=4, draw=black, fill=orange!10!white] {};

\node (fifotwo) at (4, 16) [rectangle split, rectangle split parts=4, draw=black, fill=orange!10!white] {};

\node (sqrt) at (4, 18)[circle, draw=black, fill=red!10!white] {$\sqrt{~}$};
\node (plus) at (2, 16)[circle, draw=black, fill=red!10!white] {$+$};
\node (minus) at (3, 14)[circle, draw=black, fill=red!10!white] {$-$};

\draw [->, thick] (x.south) to (times.north);
\draw [->, thick] (0, 19) -- ++(1, 0) -- ++(0, -1) -- (times.east);
\draw [->, thick] (y.south) to (fifo.north);
\draw [->, thick] (z.south) to (sqrt.north);
\draw [->, thick] (times.south) -- (0, 16) -- (plus.west);
\draw [->, thick] (fifo.south) -- (plus.north);
\draw [->, thick] (plus.south) -- (2, 14) -- (minus.west);
\draw [->, thick] (sqrt.south) -- (fifotwo.north);
\draw [->, thick] (fifotwo.south) -- (4, 14) -- (minus.east);


\node (r) at (3, 12) {$r$};
\draw [->, thick] (minus.south) -- (r.north);

\draw [dashed, thick] (4.8, 20) -- ++(0, -9);
\node (func) at (5, 21) {$r = x^2 + y - \sqrt{z}$};


\fill [orange!5!white] (8.5, 21) rectangle (11.5, 16.5);
\draw (8.5, 21) rectangle (11.5, 16.5);
\node (i1) at (10, 20.5) {\verb!load r1, x!};
\node (i2) at (10,20) {\verb!load r2, y!};
\node (i3) at (10,19.5) {\verb!load r3, z!};
\node (i4) at (10,19) {\verb!mul r1, r1!};
\node (i5) at (10,18.5) {\verb!add r1, r2!};
\node (i6) at (10,18) {\verb!sqrt r3    !};
\node (i7) at (10,17.5) {\verb!sub r1, r3!};
\node (i8) at (10, 17) {\verb!str r1, r  !};



\node (FD) at (7, 16)[draw=black, fill=blue!10!white] {Fetch/Decode};
\node (ALU) at (7, 13.5)[trapezium, draw=black, fill=blue!10!white, shape border rotate=180] {ALU ($+$ $-$ $*$ $\sqrt{~}$)};
\node (LD) at (11, 16) [rectangle, draw=black, fill=blue!10!white] {Load/Store};
\node (MEM) at (14,  16) [rectangle, draw=black, shape border rotate=90, fill=green!10!white] {CACHE/MEM};
\node (REGS) at (14, 14) [rectangle split, rectangle split parts=4, draw=black, fill=green!10!white] {REGISTERS};
\draw [->, thick] (FD.south) -- (ALU.north);
\draw [->, thick] (FD.east) -- (LD.west);
\draw [<->, thick] (LD.east) -- (MEM.west);
\draw [<->, thick] (LD.south) -- ++(0, -1.7)-- (REGS.west);
\draw [->, thick] ($(REGS.west) + (0, -0.5)$) -- (ALU.east);
\draw [->, thick] (ALU.south) -- ++(0, -2) -- ++(7,0) -- (REGS.south);
\draw [->, thick, dashed] (8.5, 19.25) to[out=180, in=90] (FD.north);
\end{tikzpicture}
\caption{Diagram showing the computation of the function $r=x^2 + y - \sqrt{z}$} by using a custom streaming datapath (left)
and using a sequence of instructions in a conventional CPU (right).
\label{fig:StreamVsCPU}
\end{figure}

\section{Performance Model}
In order to justify the design decisions made, we present a performance model that aims to predict the maximum achievable performance
increase. For this we need to take into account the hardware characteristics, such as DRAM and PCIe bandwidth and the chip clock frequency
as well as the format of the data, in particular the widths of the data sets. 
We start by considering the structure and partitioning of the mesh. In the sample meshes that we have the number
of nodes is roughly the same as the number of cells. The number of edges is about twice the number of cells/nodes.

Let $C_{tot}$ = total number of cells, $N_{tot}$ = total number of nodes, $E_{tot}$ = total number of edges.
$C_{pp}$ is the number of cells per partition and depends solely on the amount of memory available on the FPGA.
Same goes for the number of nodes per partition $N_{pp}$. $N_{pp}$ is typically equal to $C_{pp}$.
We use this number to specify the desired partition size to the METIS partitioning tool to get $n$ partitions.
For the purposes of this analysis, we assume that the partitions are roughly square in shape. By square, we mean
that they have an equal number of cells/nodes on their sides/borders. In practice the tools may deliver
arbitrary shaped partitions. Assuming square partitions of dimension $l \times l = C_{pp}$, the number of halo cells
will be $C^h = l + l + 2\times(l-1) \approx 4l = 4\times \sqrt{C_{pp}}$ as $l$ becomes large. The same for the number of halo nodes
$N^h = 4l = 4\times \sqrt{C_{pp}}$. The number of edges per partition will be $E_{pp} = E_{tot} / n$.

We can reasonably assume that partitioning the macro-partitions into two micro-partitions will give
two micro-partitions of equal size. In that case the number of cells per micro-partition is
$C_{\mu p} = C_{pp} / 2$ and the number of nodes is $N_{\mu p} = N_{pp} / 2$ and similarly for the edges
$E_{\mu p} = E_{pp} / 2$. In a similar fashion we calculate the numbers for the halo regions of each
micro-partition: $C^h_{\mu p} = C^h / 2$, $N^h_{\mu p} = N^h / 2$.
We assume the intra-partition halo to be negligible in size, which is confirmed in practice.
We represent the width of the data sets involved in res\_calc in bits:
$\overline{q} = 4\times32 = 128$, $\overline{x} = 2\times32 = 64$, $\overline{adt}=32$, $\overline{res} = 4\times32 = 128$.

Next, we take into account the hardware characteristics. The important factors are: the DRAM bandwidth ($B_D$), the
PCIe bandwidth ($B_P$) and the clock frequency $f$. Note that the DRAM bandwidth $B_D$ is shared between streams to and from
DRAM, while the PCIe streams get $B_P$ of bandwidth to the FPGA and $B_P$ from the FPGA.

Recall the phases of computation in the accelerator:
\begin{enumerate}
\item{Read in data for first micro-partition plus the intra-partition halo.
 If this is not the first macro-partition, write out the non-halo data for the second micro-partition and the intra-partition halo.}
\item{Process first micro-partition, read in the non-IPH data for second micro-partition.}
\item{Process second micro-partition, write out the non-IPH data for the first micro-partition.}
\end{enumerate}

The total execution time can be estimated by adding the execution times for each of the three phases.
So we consider each phase in turn.
\subsection{Phase 1}
The size of the input data for one cell is $\overline{C_{in}} = \overline{q} + \overline{adt} = 128 + 32 = 160$ bits.
The size of the input data for one node is just the size of the $x$ dataset: $\overline{x} = 64$.
Therefore the amount of data that needs to be transferred from DRAM is $D^{in}_{DRAM} = C_{\mu p} \times \overline{C_{in}} + N_{\mu p} \times \overline{x}$.
We are also writing back the $res$ vectors back to DRAM and PCIe from the previous micro-partition.
The width of the result is $\overline{C_{out}} = \overline{res} = 128$ and thus the amount of data to be written out
to DRAM is $D^{out}_{DRAM} = C_{\mu p} \times \overline{C_{out}}$.
Similarly the amount of halo data to be transferred from PCIe is $D^{in}_{PCIe} = C^h_{\mu p} \times \overline{C_{in}} + N^h_{\mu p} \times \overline{x}$
and the data written out to PCIe is: $D^{out}_{PCIe} = C^h_{\mu p} \times \overline{C_{out}}$.
The DRAM bandwidth is shared among both input and output streams, therefore the bandwidth allocated to the input stream from DRAM to
the kernel will be $B^{in}_D = B_D \times \dfrac{D^{in}_{DRAM}}{D^{in}_{DRAM} + D^{out}_{DRAM}}$ and the bandwidth to DRAM will be
$B^{out}_D = B_D - B^{in}_D$.
As mentioned earlier, both PCIe streams get the same bandwidth $B_P$.
Knowing the bandwidths we can calculate the times needed for the data transfers.
$t_{DRAM} = \dfrac{D^{in}_{DRAM}}{B^{in}_D} = \dfrac{D^{out}_{DRAM}}{B^{out}_D}$ for the DRAM transfer and
$t_{PCIe} = \dfrac{D^{in}_{PCIe}}{B_P}$ for the PCIe transfer since the width of the input data is greater than the width
of the output data and the bandwidths for input and output are the same.
The FPGA itself is a synchronous circuit operating at $f$ cycles per second. This means that if it wants to read a data
item from but the stream does not have anything available the kernel will stall until the memory system supplies a
data item. Since the PCIe channel is slower than the DRAM channel, we need to make sure that the PCIe accesses
happen at regular intervals and not one after the other so as to avoid forcing the DRAM to deliver data in
lockstep with the PCIe bus. Similarly for output streams. We assume such a configuration here.
We assume that we read/write one node/cell pair from DRAM every cycle and a halo node/cell pair from 
PCIe every few cycles in such a way as to not throttle the DRAM. Assuming a clock frequency $f$, the
minimum time needed to consume the data for a micro-partition will be $t_{FPGA} = \dfrac{C_{\mu p}}{f}$.
Thus, the time taken to complete phase 1 will be the maximum of the three times calculated:
$t_{1} = max(t_{DRAM}, t_{PCIe}, t_{FPGA})$. Ideally we want the three times to be in then order:
$t_{PCIe} < t_{DRAM} < t_{FPGA}$, i.e. we don't want PCIe transfer to be a bottleneck.

\subsection{Phase 2}
In phase 2, the first micro-partition gets processed, i.e. its edges are streamed in, used to address the
block RAMs and results are written to the $res$ block RAMs. Also, the second micro-partition is streamed in
and stored in local storage. Again, we stream in data of width $\overline{C_{in}}$ from both DRAM and PCIe, but this time
we also stream in edge data in the for of four addresses: one for each of the two cells and nodes. The width of
the edge data depends on the number of elements that are in a partition. A block RAM of depth/size $d$ has addresses
of width $\lceil log_2(d) \rceil$ bits. The depth of the block RAMs in our case is the number of cells/nodes in a partition
$C_{pp}$ (typically, the number of nodes and cells is roughly equal), 
therefore the width of each address is $\lceil log_2(C_{pp})\rceil$ and the width of the data for each edge is
$\overline{E_{in}} = 4 \times \lceil log_2(C_{pp})\rceil$. Therefore the amount of data we are transferring
from DRAM will be $D^{in}_{DRAM} = E_{\mu p} \times \overline{E_{in}} + C_{\mu p} \times \overline{C_{in}} + N_{\mu p} \times \overline{x}$,
while the amount of halo data transfered from PCIe will be $D^{in}_{PCIe} = C^h_{\mu p} \times \overline{C_{in}} + N^h_{\mu p} \times \overline{x}$.

As before, we calculate the times to transfer the data from DRAM $t_{DRAM} = \dfrac{D^{in}_{DRAM}}{B_D}$
and PCIe $t_{PCIe} = \dfrac{D^{in}_{PCIe}}{B_P}$.

This time, the number of cycles taken to consume the data by the kernel will be dominated by the number of edges, since there are about
twice the number of edges on average than cells or nodes. The number of edges we process per cycle is the number of 
arithmetic pipelines we have $n_p$. The minimum time taken by
the FPGA to consume all edges in a micro-partition in phase 2 will then be $t_{FPGA} = \dfrac{E_{\mu p}}{f \times n_p}$. Again, the total
runtime for this phase will be dominated by the maximum of the three times calculated above, that is $t_2 = max(t_{DRAM}, t_{PCIe}, t_{FPGA})$.
\subsection{Phase 3}
The third phase is similar to the second one, except that we are now writing back the first micro-partition and processing the second one.
The numbers remain the same for $t_{DRAM}$, $t_{PCIe}$ and $t_{FPGA}$. $t_3 = max(t_{DRAM}, t_{PCIe}, t_{FPGA})$. The total time to process
one iteration of the whole mesh is therefore: $t_{tot} = n\times(t_1 + t_2 + t_3)$. The time to execute $i$ iterations then becomes
$t^i_{tot} = i \times n \times (t_1 + t_2 + t_3)$.

\subsection{Design space exploration}
Having the above model, we can plug in values for existing test meshes and architecture options to explore the effects that each one will have
on the performance. This way we can identify bottlenecks in performance and optimise the architecture accordingly.
Substituting values for our test mesh of 721801 nodes, 720000 cells and 1438600 edges and $2^13=8192$ cells/nodes per partition for an architecture
with one arithmetic pipeline running at $f = 240$MHz we get
values for $t_1 = 1.71\times 10^{-5}s, t_2 = 3.44\times 10^{-5}s, t_3 =  3.44\times 10^{-5}s$ for a total runtime $t_{tot} = 7.48\times 10^{-3}s$.
Extrapolating to 2000 iterations, we get $t^{2000}_{tot} = 14.96s$. This estimated runtime beats the time of 53.99 seconds
for res\_calc shown in table \ref{tab:Airfoil_timesCPU}, achieving a speedup of $\times 3.6$. However, it is more interesting to compare the runtime to an accelerated version.
One such implementation, developed at the Software Performance Optimisation group at Imperial College uses the NVIDIA Tesla M2050 accelerator \cite{TeslaM2050}
programmed using the OpenCL framework \cite{OpenCL}. It uses the large number of cores present on the card to launch thousands of threads in parallel, thus
greatly accelerating computation.
The times for the various Airfoil kernels on the same test mesh are shown in table \ref{tab:Airfoil_timesCPU}:
\begin{table} [H]
  \begin{tabular}{| c | c | c |}
    \hline                        
    Kernel Name & Time spent (seconds) & Percentage of total time (\%) \\ \hline \hline
    save\_soln & 0.4593 & 4.75 \\ \hline
    adt\_calc & 1.1310 & 11.69\\ \hline
    res\_calc & 6.5122 & 67.34\\ \hline
    bres\_calc & 0.0640 & 0.66 \\ \hline
    update & 1.5042  & 15.55\\ \hline
  \end{tabular}
  \caption{Table showing the time spent in each kernel during a run of a hardware accelerated version of Airfoil on a Tesla M2050 GPU. 
The total run time is 9.67 seconds.}
  \label{tab:Airfoil_timesCPU}
\end{table}
Notice how the execution time has improved in contrast to the single-threaded CPU variant and how res\_calc now dominates the execution time.
The time we should be planning to beat is 6.51 seconds. In the graphs following, the times are reported in seconds.

\subsubsection{Number of arithmetic pipelines}
If we look at the numbers we computed for the above mesh we notice that for phases 2 and 3 of the accelerator, the time spent
computing the edges ($t_{FPGA}$) is $3.44\times 10^{-5}s$, dominating the execution of that phase, while the time for DRAM transfer is
$t_{DRAM} = 4.43\times 10^{-6}s$. This is a strong indication that the edge processing is the bottleneck in performance. Recall that
$t_{FPGA} = \dfrac{E_{\mu p}}{f \times n_p}$. $E_{\mu p}$ is a characteristic of the mesh and not subject to variation. The clock frequency
$f$ and the number of arithmetic pipelines $n_p$ we can vary. Lets start with $n_p$. Increasing the number of pipelines we get a considerable
boost in performance, as shown in figure \ref{fig:Pipelines}.

\begin{figure}[H]
  \centering
    \includegraphics[width=1\textwidth]{graphs/pipelines.pdf}
  \caption{Plot of estimated execution time for 2000 iterations of res\_calc against the number of arithmetic pipelines in the architecture. The clock frequency
is set to 240MHz.
\label{fig:Pipelines}}
\end{figure}
Notice how after 8 pipelines we do not get a performance boost anymore because the bottleneck now shifts to the memory transfers, DRAM
in particular. Using just 4 pipelines our architecture is estimated to beat the Tesla performance, which is an impressive result, considering
the hundreds of concurrent cores running on the Tesla.

\subsubsection{Clock frequency}
Next we explore the effects of varying the clock frequency of the chip. Realistically, the frequencies achievable on the hardware range
from 120MHz to 300MHz. The results of increasing the frequency are presented in figure \ref{fig:frequencies}. While frequencies above 300MHz
are not in general achievable, we present the increase it would take to catch up to the Tesla implementation. As we can see, raising the clock
frequency is clearly a worse choice than increasing the number of pipelines. Note also that raising the clock frequency implies more energy
consumption leading to more heat dissipation, further reducing the temptation to raise it.
\begin{figure}[H]
  \centering
    \includegraphics[width=1\textwidth]{graphs/frequencies.pdf}
  \caption{Plot of estimated execution time for 2000 iterations of res\_calc against the clock frequency of the FPGA using one arithmetic pipeline.
\label{fig:frequencies}}
\end{figure}

\subsubsection{Clock frequency and arithmetic pipelines}
It is interesting to see the effects of varying the frequency $f$ as well as the number of arithmetic pipelines $n_p$. The results of such an experimentation
are shown in figures \ref{fig:frequencies_pipelines} and \ref{fig:frequencies_pipelines3D}. 
\begin{figure}[H]
  \centering
    \includegraphics[angle=-90, width=0.8\textwidth]{graphs/frequencies_pipelines.pdf}
  \caption{Plot of estimated execution time for 2000 iterations of res\_calc against the clock frequency of the FPGA for various numbers of arithmetic pipelines.
\label{fig:frequencies_pipelines}}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[width=0.8\textwidth]{graphs/freq_pipes3D.pdf}
  \caption{Plot of estimated execution time for 2000 iterations of res\_calc against the clock frequency of the FPGA for various numbers of arithmetic pipelines in a
  3-dimensional plot.
\label{fig:frequencies_pipelines3D}}
\end{figure}

We see that by using 6 or 7 pipelines and clocking the design at 300MHz, we can beat the Tesla implementation by about 66\%, offering a total of
$\times 13.8$ speedup over the serial CPU version. These numbers validate the architecture we designed by providing some estimates about the
potential speedup. Having performed this performance analysis, we can now move on to implementing the architecture in order to discover potential
issues that might prevent us from achieving the maximum performance that we predict with this model.

\subsubsection{Partition size}
Once we remove the arithmetic pipeline bottleneck by adding more pipelines we can experiment with partition sizes to see the effects. varying the 
number of cells per partition $C_{pp}$ gives us the data shown in figure \ref{fig:partition_sizes}. Note, also the ratio of non-halo to halo transfers
in figure \ref{fig:partition_ratios}.

\begin{figure}[h]
\centering
    \includegraphics[width=1\textwidth]{graphs/partition_sizes.pdf}
  \caption{Plot of estimated execution time for 2000 iterations of res\_calc against the number of cells per partition. Note that the horizontal axis
  is on a log-2 scale. The design has 8 pipelines and runs at 240MHz.}
  \label{fig:partition_sizes}
\end{figure}
 \begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{graphs/partition_ratios.pdf}
  \caption{Plot of estimated DRAM to PCIe transfer time ratio ($\dfrac{t_{DRAM}}{t_{PCIe}}$) against the number of cells per partition. Note that the horizontal axis
  is on a log-2 scale.The design has 8 pipelines and runs at 240MHz.}
  \label{fig:partition_ratios}
\end{figure}

We notice that for values of $C_{pp}$ less than 2048 the PCIe transfer time dominates execution time. For values of $C_{pp} > 8192$ we can see
that the edge computations become the bottleneck once again. Notice how steeply the performance degrades when PCIe transfers dominate.
This result is to be expected since the size of the halo region is proportional
to the square root of the partition size, in other words $C^h_{pp} \propto \sqrt{C_{pp}}$.

\chapter{Implementation}
In this chapter we present the issues that arise when trying to implement the FPGA-based accelerator and the host-side mesh
preprocessing code. Due to time limitations we attempt to build a design using one arithmetic pipeline with 8192 ($2^13$) cells
per partition.

\section{Mesh partitioning}
We use the METIS partitioning tool through its C API to assign a partition number to each cell and node. This is where we use
the indirection maps that define the connectivity. When assigning edges to partitions we must take care to also add the nodes and
edges that they reference from neighbouring partitions. These nodes and cells will be part of the halo region.

Next, we need to perform the second level of partitioning. For this we need to translate the global indirection maps (\emph{cell, edge} etc)
into local maps for each partition. For this we need to remap the nodes and cells to a local numbering. We use hash maps to store the associations
between global and local numbering and use them to translate the \emph{cell} indirection map into local maps for each partition.

Now we can perform the second level of partitioning by breaking up each partition into 2 micro-partitions. We actually want three regions
from this process: the two micro-partitions and the intra-partition halo (\emph{IPH}). We assign edges to each partition again
taking care to add the nodes and cells from the neighbouring micro-partition. Since METIS provides a partition number for each cell and node,
we need to figure out the IPH region ourselves. We store the nodes for each micro-partition into a set (we used a hash set but any implementation
could be used) called $Nodes_{\mu p1}$ and $Nodes_{\mu p2}$ and calculate the IPH region by $Nodes_{IPH} = Nodes_{\mu p1} \cap Nodes_{\mu p2}$. Now
we need to remove the common nodes from $Nodes_{\mu p1}$ and $Nodes_{\mu p2}$, that is $Nodes_{\mu p1} := Nodes_{\mu p1} \backslash Nodes_{IPH}$ and
$Nodes_{\mu p2} := Nodes_{\mu p2} \backslash Nodes_{IPH}$, where $A \backslash B$ is the set $A$ with the elements of set $B$ removed ($\backslash$ is the set difference operator).
 We follow the same procedure for cells and we arrive at the partitions that we need.

\section{Edge scheduling}
Recall the use of the accumulator that adds the increments computed by the arithmetic pipeline to the current values of $res$ (figure \ref{fig:Architecture1st}), creating
a loop in the architecture.
This has implications on the order with which we process the edges. Suppose the accumulator has latency $l$, that is if the operands for addition
arrive to it at cycle $c$ the corresponding result will appear on its output at cycle $c + l$. That latency depends on the number of pipeline stages in the adder.
One of the operands is the increment computed by the
arithmetic pipeline and the other is the previous value of $res$ that was read from the corresponding BRAM. The new $res$ result that must be written
back to the BRAMs will be produced in $l$ cycles. This means that within that window we must not access the same BRAM address (i.e. the same cell) because
we will have not commited the previous result of $res$ that is still in the accumulator pipeline, thus getting two values for $res$ that will be commited to
the BRAM with the earliest one being overwritten instead of being added. This outcome is shown in figure \ref{fig:edge_dependency} where two values $v_1$ and $v_4$
for the cell at $\alpha_0$ are being computed. When $v_4$ exits the pipeline it will be written at address $\alpha_0$ in the BRAM, only to be overriden by $v_1$ 3 cycles
later. To work around this issue we constrain the iteration order of the edges in each micro-partition. The constraint is that for every edge no edge in a window of
width $l$ must access the any of the two cells. Thus, given an ordering of $n$ edges $sch$ the validity of the schedule $sch$ for a window width $l$ can be determined
by algorithm \ref{alg:valid_schedule}.

\begin{figure}[H]
\centering
\begin{tikzpicture}
\matrix (acc) at (10,10)
[
  draw,
  inner sep=0,
  matrix of nodes,
  fill=orange!10!white,
  every node/.style={
    inner sep=1ex
  }
]
{
  \node{\textcolor{red}{$v_1@\alpha_{0}$}};\\
  \hline
  \node{$v_2@\alpha_{1}$};\\
  \hline
  \node{$v_3@\alpha_{2}$};\\
  \hline
  \node{\textcolor{red}{$v_4@\alpha_{0}$}};\\
  \hline
  \node{$v_5@\alpha_{3}$};\\
  \hline
  \node{$v_6@\alpha_{4}$};\\
};

\draw [<->, dashed, thick] ($(acc.north) + (1, 0)$) -- ($(acc.south) + (1, 0)$);
\node at ($(acc) + (1.5, 0)$) {\emph{$l=6$}};
\draw [black,decorate,decoration={brace,amplitude=4pt}]
($(acc.south) + (-1, 0)$) -- ($(acc.north) + (-1, 0)$) node [black,midway,xshift=-0.5cm] 
{\Large \textbf{$+$}};
% \node at ($(acc) + (-1, 0)$) {\Large \emph{$+$}};

\node (bram) at (10, 6.5) [draw, fill=red!10!white] {\emph{BRAM}};
\node (arith) at (10, 14) [draw, fill=green!10!white] {\emph{ARITH}};
\draw [->, thick] (acc.south) -- (bram.north);
\draw [->, thick] ($(arith.south) + (-0.2, 0)$) -- ($(acc.north) + (-0.2, 0)$);
\draw [->, thick] (bram.east) -- ++(1.75, 0) -- ++(0, 6) -- ($(acc.north) + (0.25, 0.5)$) -- ($(acc.north) + (0.25, 0)$);

\end{tikzpicture}
\caption{Architecture diagram of the accumulation part of the accelerator design showing the conflicting values of $res$ being
computed in the accumulator pipeline. The conflicting values for address $\alpha_{0}$ are shown in red.}
\label{fig:edge_dependency}
\end{figure}

\newfloat{program}{thp}{prog}
\floatname{program}{Algorithm}

\begin{program}[H]
 \begin{algorithmic}
 \Function{$boolean$ validSchedule} {$sch[~]$, $n$, $l$}
   \For{$i$ in $[0..n-1]$}
     \State $C_i \gets$ set of cells referenced by edge $sch[i]$
     \For{$j\gets 1$ ; $j < l$ ; $j\gets j+1$}
       \State $C_{(i+j)\%n}\gets$ set of cells referenced by edge $sch[(i+j)\%n]$
       \If{$C_i \cap C_{(i+j)\%n} \neq \emptyset$}
         \State\Return $False$
       \EndIf
     \EndFor
   \EndFor
   \State \Return $True$
 \EndFunction
 \end{algorithmic}
\caption{Pseudocode that validates an edge schedule $sch$ of $n$ edges with window width $l$.}
\label{alg:valid_schedule}
\end{program}

In order to compute that schedule we partition each micro-partition again into multiple partitions, calling them
\emph{edge partitions}.
We define two edge partitions being \emph{adjacent} if an edge in one references a cell in the other. Using
this definition we construct an \emph{adjacency graph} where the nodes are the edge partitions and we connect two
nodes with an edge if the two edge partitions are adjacent. That way we know that edges belonging to two
non-adjacent partitions will never conflict and can be scheduled one after the other. That way, the problem
of finding a schedule for the edges can be reduced to finding a schedule for the nodes in the adjacency graph
such that no two nodes within a window of width of width $l$ will be adjacent. An example is shown in figure
\ref{fig:parts_graph}. Using the adjacency graph we can transform algorithm \ref{alg:valid_schedule} for checking
the validity of a schedule into algorithm \ref{alg:valid_schedule_graph}.


\begin{figure}[h]
\begin{tikzpicture}[scale=0.7]
  \fill[green!10!white] (0, 0) rectangle (10,10);
  \draw[step=0.5cm] (0, 0) grid (10, 10);

  \fill [blue!40!white] (0, 0) rectangle (10,10);
  \fill [orange!20!white] (0, 0) rectangle (3, 4);
  \fill [green!20!white] (3, 4) rectangle (0, 7.5);
  \fill [magenta!20!white] (0, 10) rectangle (3, 7.5);
  \fill [red!40!white] (3, 7.5) rectangle (7, 4);
  \fill [purple!20!white] (7, 7.5) rectangle (10,10);
  \fill [orange!40!white] (5, 0) rectangle (7, 4);

  \fill [yellow!20!white] (7, 0) rectangle (10, 4);
  \fill [olive!40!white] (3, 0) rectangle (5, 4);
  \fill [violet!40!white] (5, 7.5) rectangle (7, 10);
  \fill [blue!20!white] (7, 4) rectangle (10, 7.5);

  \draw[step=0.5cm] (0, 0) grid (10, 10);

  \draw [red, very thick] (0, 0) rectangle (10,10);

  \draw [red, very thick] (0, 0) rectangle (3, 4);
  \node at (1.5, 2) [fill=orange!20!white]{\Large 1};

  \draw [red, very thick] (3, 4) rectangle (0, 7.5);
  \node at (1.5, 5.75) [fill=green!20!white]{\Large 2};

  \draw [red, very thick] (0, 10) rectangle (3, 7.5);
  \node at (1.5, 8.75) [fill=magenta!20!white]{\Large 3};

  \draw [red, very thick] (3, 7.5) rectangle (7, 4);
  \node at (5, 5.75) [fill=red!40!white]{\Large 4};

  \draw [red, very thick] (7, 7.5) rectangle (10,10);
  \node at (8.5, 8.75) [fill=purple!20!white]{\Large 5};

  \draw [red, very thick] (5, 0) rectangle (10, 4);
  \node at (8.5, 2) [fill=yellow!20!white]{\Large 6};

  \node at (4, 2) [fill=olive!40!white]{\Large 7};
  \node at (8.5, 5.75) [fill=blue!20!white]{\Large 8};

  \node at (6, 8.75)[fill=violet!40!white] {\Large 9};

  \draw [red, very thick] (5, 0) rectangle (7, 4);
  \node at (6, 2)[fill=orange!40!white] {\Large 10};

  \node at (4, 8.75) [fill=blue!40!white]{\Large 11};

  \draw [red, very thick] (3, 7.5) rectangle (5, 10);


  \draw [thick, dashed] (11, 11) -- (11, -1);

  \node (one) at (14, 10) [circle, draw, fill=orange!20!white] {\Large 1};
  \node (two) at (16, 8) [circle, draw, fill=green!20!white] {\Large 2};
  \node (three) at (20, 4) [circle, draw, fill=magenta!20!white] {\Large 3};
  \node (four) at (16, 6) [circle, draw, fill=red!40!white] {\Large 4};
  \node (five) at (18, 6) [circle, draw, fill=purple!20!white] {\Large 5};
  \node (six) at (14.5, 3) [circle, draw, fill=yellow!20!white] {\Large 6};
  \node (seven) at (12, 6) [circle, draw, fill=olive!40!white] {\Large 7};
  \node (eight) at (17, 4) [circle, draw, fill=blue!20!white] {\Large 8};
  \node (nine) at (18.5, 4) [circle, draw, fill=violet!40!white] {\Large 9};
  \node (ten) at (12, 2) [circle, draw, fill=orange!40!white] {\Large 10};
  \node (eleven) at (18.5, 2) [circle, draw, fill=blue!40!white] {\Large 11};

  \draw [thick](one) -- (seven);
  \draw [thick](one) -- (two);
  \draw [thick](two) -- (four);
  \draw [thick](two.east) to[out=0, in=90] (three.north);
  \draw [thick](three) -- (eleven);
  \draw [thick](four) -- (seven);
  \draw [thick](four) -- (eight);
  \draw [thick](four) -- (nine);
  \draw [thick](four) -- (ten);
  \draw [thick](four.south) to[out=270, in=180] (eleven.west);
  \draw [thick](five) -- (eight);
  \draw [thick](five) -- (nine);
  \draw [thick](six) -- (eight);
  \draw [thick](six) -- (ten);
  \draw [thick](seven) -- (ten);
  \draw [thick](nine) -- (eleven);
\end{tikzpicture}
\caption{An example partitioning of a micro-partition into 11 edge partitions (left) and the adjacency graph generated 
from that partitioning (right).}
\label{fig:parts_graph}
\end{figure}

\begin{program}[H]
 \begin{algorithmic}
 \Function{$boolean$ validGraphSchedule} {$sch[~]$, $n$, $l$, $g$}
   \For{$i$ in $[0..n-1]$}
     \For{$j:=1$ ; $j < l$ ; $j:=j+1$}
       \If{$sch[i]$ adjacent to $sch[(i+j)\%n]$ in $g$}
         \State\Return $False$
       \EndIf
     \EndFor
   \EndFor
   \State \Return $True$
 \EndFunction
 \end{algorithmic}
\caption{Pseudocode that validates a node schedule $sch$ of an adjacency graph $g$ with
$n$ elements for a window width of $l$.}
\label{alg:valid_schedule_graph}
\end{program}

To actually compute a schedule given an adjacency graph and a window width, we can use 
algorithm \ref{alg:schedule_algorithm}. Note that the $sch$ function is recursive and 
uses the $validPos$ function defined in algorithm \ref{alg:schedule_validPos} to
pick which nodes to attempt to schedule.
Due to the high number of adjacency checks, we choose to represent the graphs
as two-dimensional adjacency matrices $mat$ where $mat[i][j] = 1$ if nodes $i$ and $j$
are adjacent and $max[i][j] = 0$ otherwise. Algorithm \ref{alg:schedule_algorithm} performs a 
depth-first search of the state space, backtracking to try different paths if a particular
path fails thanks to the use of queues to store nodes that should be tried next. The recursion
handles the backtracking. At every call of $sch$ the algorithm will try to place a node into the
schedule and determine which of its

\begin{program}[H]
 \begin{algorithmic}
 \Function{$node[~]$ scheduleGraph} {Graph $g$, int $l$}
  \State $nnodes \gets $ number of nodes in $g$
  \State $res \gets node[nnodes]$ \Comment{array of size $nnodes$}
  \State $count \gets 0$
  \State $q \gets empty$ $Queue$
  \ForAll{$node$ in $g$}
    \State enqueue $node$ in $q$
  \EndFor
  \While{$q$ not empty}
    \State $n \gets q.dequeue()$
    \If{\Call{sch}{$n, \&count, \&res, g,$}}
      \State \Return $sch$
    \EndIf
  \EndWhile
  \State \Call{print}{"Could not schedule graph"}
  \State \Return $NULL$
  \EndFunction

  \Function{$boolean$ sch}{node $n$, node[~] $res$, int $count$, Graph $g$, int $l$}
    \State $res[count] \gets n$
    \State $count \gets count + 1$
    \State $q \gets empty$ $Queue$
    \If{$count =$ number of nodes in $g$}
      \State \Return $TRUE$
    \EndIf
    \ForAll{$nn$ in $g$ and not adjacent to $n$}
      \If{$nn \notin res~ \wedge$ \Call{validPos}{$res, nn, count, l, g$}}
        \State enqueue $nn$ in $q$
      \EndIf
    \EndFor
    \While{$q$ not empty}
      \State $child \gets q.dequeue()$
      \If{\Call{sched}{child, \&res, \&count, g, l}}
        \State \Return $TRUE$
      \EndIf
    \EndWhile
    \State $count \gets count - 1$ \Comment{Failed to find schedule down this path}
    \State \Return $FALSE$
  \EndFunction

 \end{algorithmic}
\caption{Pseudocode that computes a schedule for the nodes in a graph $g$ with window width $l$.}
\label{alg:schedule_algorithm}
\end{program}

\begin{program}
\begin{algorithmic}
  \Function{$boolean$ validPos}{node[~] $arr$, node $n$, int $c$, int $l$, Graph $g$}
    \If{$c \leq l$}
      \For{$p \gets 0$ ; $p < count$ ; $++p$}
        \If{$arr[p]$ adjacent to $n$ in $g$}
          \State \Return $FALSE$
        \EndIf
      \EndFor
    \EndIf
    \State $nnodes \gets$ number of nodes in $g$
    \If{$c > nnodes - l$}
      \For{$p \gets 0$ ; $p < l - (nnodes - c)$ ; $p \gets p+1$}
        \If{$arr[p]$ adjacent to $n$ in $g$}
          \State \Return $FALSE$
        \EndIf
      \EndFor
    \EndIf
    \For{$p \gets c - l - 1$ ; $p < c$ ; $p\gets p + 1$}
      \If{$arr[p]$ adjacent to $n$ in $g$}
        \State \Return $FALSE$
      \EndIf
    \EndFor
  \State \Return $TRUE$
  \EndFunction
\end{algorithmic}
\caption{Pseudocode that checks whether inserting node $n$ at position $c$ into the
schedule $arr$ will produce a valid partial schedule for window width $l$ of the nodes in
graph $g$.}
\label{alg:schedule_validPos}
\end{program}

 
% \chapter{Evaluation}
% In this section we present the evaluation strategy for our approach. We explain the alternative
% implementations of Airfoil and contrast their performance with our scheme. We test the various
% imlementations against progressively larger mesh sizes and present and interpret the results.

% \chapter{Conclusion and further work}
\begin{thebibliography}{9}

\bibitem{OP2_presentation}
  MB Giles, GR Mudalige, Z Sharif, G Markall, PHJ Kelly,\\
  \emph{Performance Analysis of the OP2 Framework on Many-core Architectures}.\\
  ACM SIGMETRICS Performance Evaluation Review, 38(4):9-15, March 2011

\bibitem{OP2_Cluster}
  G.R Mudalige, MB Giles, C. Bertolli, P.H.J. Kelly,\\
  \emph{Predictive Modeling and Analysis of OP2 on DistributedMemory GPU Clusters}\\
  PMBS '11 Proceedings of the second international workshop on Performance modeling, benchmarking and simulation of high performance computing systems
  Pages 3-4 

\bibitem{Virtex6Spec}
  Xilinx Inc.\\
  \emph{Virtex-6 Family Overview}\\
  http://www.xilinx.com/support/documentation/virtex-6.htm

\bibitem{MaxCompiler_whitepaper}
  Maxeler Technologies\\
  \emph{MaxCompiler White Paper}\\
  http://www.maxeler.com/content/briefings/MaxelerWhitePaperMaxCompiler.pdf

\bibitem{METISPaper}
 G. Karypis, V. Kumar. \\
  \emph{A Fast and Highly Quality Multilevel Scheme for Partitioning Irregular Graphs}\\
  SIAM Journal on Scientific Computing, Vol. 20, No. 1, pp. 359—392, 1999.

\bibitem{IEEEFP}
  IEEE \\
  \emph{IEEE Std 754-2008 } \\
  Publication Year: 2008 , Page(s): 1 - 58

\bibitem{GhostCellPaper}
  F. B. Kjolstad, M Snir\\
  \emph{Ghost Cell Pattern}\\
  ParaPLoP '10 Proceedings of the 2010 Workshop on Parallel Programming Patterns

\bibitem{UnstructuredMeshCCM}
  M. T. Jones, K. Ramachandran\\
  \emph{Unstructured mesh computations on CCMs}\\
  Advances in Engineering Software - Special issue on large-scale analysis, design and intelligent   
synthesis environments Volume 31 Issue 8-9, Aug-Sept. 2000 

\bibitem{MemoryHierarchy}
  H. Morishita, Y. Osana, N. Fujita, H. Amano\\
  \emph{Exploiting memory hierarchy for a Computational Fluid Dynamics accelerator on FPGAs}\\
  ICECE Technology, 2008. FPT 2008. pp 193 - 200 

\bibitem{SpanishFPGAAirfoil}
  Sanchez-Roman, D.;   Sutter, G.;   Lopez-Buedo, S.;   Gonzalez, I.;   Gomez-Arribas, F.J.;   Aracil, J.;   Palacios, F.;\\
  \emph{High-Level Languages and Floating-Point Arithmetic for FPGABased CFD Simulations}\\
   Design \& Test of Computers, IEEE, 2011, Volume: 28 Issue:4, pp 28 - 37

\bibitem{SpanishFPGAAirfoil2}
  Sanchez-Roman, D.; Sutter, G.; Lopez-Buedo, S.; Gonzalez, I.; Gomez-Arribas, F.J.; Aracil, A.;\\
  \emph{An Euler Solver Accelerator in FPGA for computational fluid dynamics applications}\\
  Proceedings of the 2011 VII Southern Conference on Programmable Logic Córdoba, Argentina April 13 – 15, 2011

\bibitem{ElectromagneticsFPGA}
  Durbano, J.P.; Ortiz, F.E.;\\
  \emph{FPGA-based acceleration of the 3D finite-difference time-domain method}\\
   12th Annual IEEE Symposium on Field-Programmable Custom Computing Machines, 2004. FCCM 2004.

\bibitem{MeshIntensityROSE}
White B., McKee S. ,de Supinski B., Miller B., Quinlan D., Schulz M., Lawrence Livermore National Laboratory\\
\emph{Improving the computational intensity of unstructured mesh applications}\\
ICS '05 Proceedings of the 19th annual international conference on Supercomputing
Pages 341 - 350 

\bibitem{MeshCache}
  Sung-Eui, Y., Lindstrom, P. \\
  \emph{Mesh Layouts for Block-Based Caches}\\
  IEEE Transactions on visualization and computer graphics, Vol. 12, No. 5, September/October 2006

\bibitem{floatFPGA}
Shirazi, N., Walters, A., Athanas, P.\\
\emph{Quantitative analysis of floating point arithmetic on FPGA based custom computing machines}\\
IEEE Symposium on FPGAs for Custom Computing Machines, 1995. Proceedings.

\bibitem{Pipeline}
Hartstein, A. Puzak, Thomas R.\\
\emph{The Optimum Pipeline Depth for a Microprocessor}\\
ISCA '02 Proceedings of the 29th annual international symposium on Computer architecture
Pages 7 - 13.

\bibitem{OpenCL}
Khronos Group\\
\emph{The OpenCL Specification v1.2}\\
www.khronos.org/registry/cl/specs/opencl-1.2.pdf

\bibitem{TeslaM2050}
NVIDIA\\
\emph{Tesla M2050 / M2070 GPU Module Specification Document}\\
http://www.nvidia.com/docs/IO/43395/BD-05238-001\_v03.pdf

\end{thebibliography}
\chapter{Appendix}
\section{Airfoil Kernel definitions in C}
Even though we focused on accelerating the res\_calc kernel, the other kernels are shown here for the sake of completeness.
\begin{minted}{c}
void adt_calc(float *x1,float *x2,float *x3,float *x4,float *q,float *adt){
  float dx,dy, ri,u,v,c;
  ri = 1.0f/q[0];
  u = ri*q[1];
  v = ri*q[2];
  c = sqrt(gam*gm1*(ri*q[3]-0.5f*(u*u+v*v)));
  dx = x2[0] - x1[0];
  dy = x2[1] - x1[1];
  *adt = fabs(u*dy-v*dx) + c*sqrt(dx*dx+dy*dy);
  dx = x3[0] - x2[0];
  dy = x3[1] - x2[1];
  *adt += fabs(u*dy-v*dx) + c*sqrt(dx*dx+dy*dy);
  dx = x4[0] - x3[0];
  dy = x4[1] - x3[1];
  *adt += fabs(u*dy-v*dx) + c*sqrt(dx*dx+dy*dy);
  dx = x1[0] - x4[0];
  dy = x1[1] - x4[1];
  *adt += fabs(u*dy-v*dx) + c*sqrt(dx*dx+dy*dy);
  *adt = (*adt) / cfl;
}
\end{minted}

\begin{minted}{c}
void bres_calc(float *x1, float *x2, float *q1,
                      float *adt1,float *res1, int *bound) {
  float dx,dy,mu, ri, p1,vol1, p2,vol2, f;
  dx = x1[0] - x2[0];
  dy = x1[1] - x2[1];
  ri = 1.0f/q1[0];
  p1 = gm1*(q1[3]-0.5f*ri*(q1[1]*q1[1]+q1[2]*q1[2]));
  if (*bound==1) {
    res1[1] += + p1*dy;
    res1[2] += - p1*dx;
  }
  else {
    vol1 = ri*(q1[1]*dy - q1[2]*dx);

    ri = 1.0f/qinf[0];
    p2 = gm1*(qinf[3]-0.5f*ri*(qinf[1]*qinf[1]+qinf[2]*qinf[2]));
    vol2 = ri*(qinf[1]*dy - qinf[2]*dx);

    mu = (*adt1)*eps;

    f = 0.5f*(vol1* q1[0] + vol2* qinf[0] ) + mu*(q1[0]-qinf[0]);
    res1[0] += f;
    f = 0.5f*(vol1* q1[1] + p1*dy + vol2* qinf[1] + p2*dy) + mu*(q1[1]-qinf[1]);
    res1[1] += f;
    f = 0.5f*(vol1* q1[2] - p1*dx + vol2* qinf[2] - p2*dx) + mu*(q1[2]-qinf[2]);
    res1[2] += f;
    f = 0.5f*(vol1*(q1[3]+p1) + vol2*(qinf[3]+p2) ) + mu*(q1[3]-qinf[3]);
    res1[3] += f;
  }
}
\end{minted}

\begin{minted}{c}
void res_calc(float *x1, float *x2, float *q1, float *q2,
                     float *adt1,float *adt2,float *res1,float *res2) {
  
  float dx,dy,mu, ri, p1,vol1, p2,vol2, f;

  dx = x1[0] - x2[0];
  dy = x1[1] - x2[1];

  ri = 1.0f/q1[0];
  p1 = gm1*(q1[3]-0.5f*ri*(q1[1]*q1[1]+q1[2]*q1[2]));
  vol1 = ri*(q1[1]*dy - q1[2]*dx);

  ri = 1.0f/q2[0];
  p2 = gm1*(q2[3]-0.5f*ri*(q2[1]*q2[1]+q2[2]*q2[2]));
  vol2 = ri*(q2[1]*dy - q2[2]*dx);

  mu = 0.5f*((*adt1)+(*adt2))*eps;

  f = 0.5f*(vol1* q1[0] + vol2* q2[0] ) + mu*(q1[0]-q2[0]);
  res1[0] += f;
  res2[0] -= f;
  f = 0.5f*(vol1* q1[1] + p1*dy + vol2* q2[1] + p2*dy) + mu*(q1[1]-q2[1]);
  res1[1] += f;
  res2[1] -= f;
  f = 0.5f*(vol1* q1[2] - p1*dx + vol2* q2[2] - p2*dx) + mu*(q1[2]-q2[2]);
  res1[2] += f;
  res2[2] -= f;
  f = 0.5f*(vol1*(q1[3]+p1) + vol2*(q2[3]+p2) ) + mu*(q1[3]-q2[3]);
  res1[3] += f;
  res2[3] -= f;
}
\end{minted}

\begin{minted}{c}
void save_soln(float *q, float *qold){
  for (int n=0; n<4; n++) qold[n] = q[n];
}
\end{minted}

\begin{minted}{c}
void update(float *qold, float *q, float *res, float *adt, float *rms){
  float del, adti;

  adti = 1.0f/(*adt);

  for (int n=0; n<4; n++) {
    del = adti*res[n];
    q[n] = qold[n] - del;
    res[n] = 0.0f;
    *rms += del*del;
  }
}
\end{minted}


\end{document}